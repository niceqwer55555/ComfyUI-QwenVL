# ComfyUI-QwenVL (GGUF) - ä¼˜åŒ–ç‰ˆï¼šé»˜è®¤å¯ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œæ”¯æŒå¤šå›¾åˆ†æ

import base64
import gc
import io
import inspect
import json
import os
import time
from dataclasses import dataclass
from pathlib import Path

import numpy as np
import torch
from huggingface_hub import hf_hub_download
from PIL import Image

import folder_paths
from AILab_OutputCleaner import OutputCleanConfig, clean_model_output

NODE_DIR = Path(__file__).parent
CONFIG_PATH = NODE_DIR / "hf_models.json"
SYSTEM_PROMPTS_PATH = NODE_DIR / "AILab_System_Prompts.json"
GGUF_CONFIG_PATH = NODE_DIR / "gguf_models.json"


def _load_prompt_config():
    preset_prompts = ["ğŸ–¼ï¸ Detailed Description"]
    system_prompts: dict[str, str] = {}

    try:
        with open(CONFIG_PATH, "r", encoding="utf-8") as fh:
            data = json.load(fh) or {}
        preset_prompts = data.get("_preset_prompts") or preset_prompts
        system_prompts = data.get("_system_prompts") or system_prompts
    except Exception as exc:
        print(f"[QwenVL] Config load failed: {exc}")

    try:
        with open(SYSTEM_PROMPTS_PATH, "r", encoding="utf-8") as fh:
            data = json.load(fh) or {}
        qwenvl_prompts = data.get("qwenvl") or {}
        preset_override = data.get("_preset_prompts") or []
        if isinstance(qwenvl_prompts, dict) and qwenvl_prompts:
            system_prompts = qwenvl_prompts
        if isinstance(preset_override, list) and preset_override:
            preset_prompts = preset_override
    except FileNotFoundError:
        pass
    except Exception as exc:
        print(f"[QwenVL] System prompts load failed: {exc}")

    return preset_prompts, system_prompts


PRESET_PROMPTS, SYSTEM_PROMPTS = _load_prompt_config()


@dataclass(frozen=True)
class GGUFVLResolved:
    display_name: str
    repo_id: str | None
    alt_repo_ids: list[str]
    author: str | None
    repo_dirname: str
    model_filename: str
    mmproj_filename: str | None
    context_length: int
    image_max_tokens: int
    n_batch: int
    gpu_layers: int
    top_k: int
    pool_size: int


def _resolve_base_dir(base_dir_value: str) -> Path:
    base_dir = Path(base_dir_value)
    if base_dir.is_absolute():
        return base_dir
    return Path(folder_paths.models_dir) / base_dir


def _safe_dirname(value: str) -> str:
    value = (value or "").strip()
    if not value:
        return "unknown"
    return "".join(ch for ch in value if ch.isalnum() or ch in "._- ").strip() or "unknown"


def _model_name_to_filename_candidates(model_name: str) -> set[str]:
    raw = (model_name or "").strip()
    if not raw:
        return set()
    candidates = {raw, f"{raw}.gguf"}
    if " / " in raw:
        tail = raw.split(" / ", 1)[1].strip()
        candidates.update({tail, f"{tail}.gguf"})
    if "/" in raw:
        tail = raw.rsplit("/", 1)[-1].strip()
        candidates.update({tail, f"{tail}.gguf"})
    return candidates


def _load_gguf_vl_catalog():
    """åŠ è½½GGUFæ¨¡å‹é…ç½®"""
    if not GGUF_CONFIG_PATH.exists():
        return {"base_dir": "LLM/GGUF", "models": {}}
    try:
        with open(GGUF_CONFIG_PATH, "r", encoding="utf-8") as fh:
            data = json.load(fh) or {}
    except Exception as exc:
        print(f"[QwenVL] gguf_models.json load failed: {exc}")
        return {"base_dir": "LLM/GGUF", "models": {}}

    base_dir = data.get("base_dir") or "LLM/GGUF"

    flattened: dict[str, dict] = {}

    repos = data.get("qwenVL_model") or data.get("vl_repos") or data.get("repos") or {}
    seen_display_names: set[str] = set()
    for repo_key, repo in repos.items():
        if not isinstance(repo, dict):
            continue
        author = repo.get("author") or repo.get("publisher")
        repo_name = repo.get("repo_name") or repo.get("repo_name_override") or repo_key
        repo_id = repo.get("repo_id") or (f"{author}/{repo_name}" if author and repo_name else None)
        alt_repo_ids = repo.get("alt_repo_ids") or []

        defaults = repo.get("defaults") or {}
        mmproj_file = repo.get("mmproj_file")
        model_files = repo.get("model_files") or []

        for model_file in model_files:
            display = Path(model_file).name
            if display in seen_display_names:
                display = f"{display} ({repo_key})"
            seen_display_names.add(display)
            flattened[display] = {
                **defaults,
                "author": author,
                "repo_dirname": repo_name,
                "repo_id": repo_id,
                "alt_repo_ids": alt_repo_ids,
                "filename": model_file,
                "mmproj_filename": mmproj_file,
            }

    legacy_models = data.get("models") or {}
    for name, entry in legacy_models.items():
        if isinstance(entry, dict):
            flattened[name] = entry

    return {"base_dir": base_dir, "models": flattened}


GGUF_VL_CATALOG = _load_gguf_vl_catalog()


def _filter_kwargs_for_callable(fn, kwargs: dict) -> dict:
    try:
        sig = inspect.signature(fn)
    except Exception:
        return dict(kwargs)

    params = list(sig.parameters.values())
    if any(p.kind == inspect.Parameter.VAR_KEYWORD for p in params):
        return dict(kwargs)

    allowed: set[str] = set()
    for p in params:
        if p.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD, inspect.Parameter.KEYWORD_ONLY):
            allowed.add(p.name)
    return {k: v for k, v in kwargs.items() if k in allowed}


def _tensor_to_base64_png(tensor) -> str | None:
    """å°†å¼ é‡è½¬æ¢ä¸ºbase64 PNGå›¾åƒ"""
    if tensor is None:
        return None
    if tensor.ndim == 4:
        tensor = tensor[0]
    array = (tensor * 255).clamp(0, 255).to(torch.uint8).cpu().numpy()
    pil_img = Image.fromarray(array, mode="RGB")
    buf = io.BytesIO()
    pil_img.save(buf, format="PNG")
    return base64.b64encode(buf.getvalue()).decode("utf-8")


def _sample_video_frames(video, frame_count: int):
    """é‡‡æ ·è§†é¢‘å¸§"""
    if video is None:
        return []
    if video.ndim != 4:
        return [video]
    total = int(video.shape[0])
    frame_count = max(int(frame_count), 1)
    if total <= frame_count:
        return [video[i] for i in range(total)]
    idx = np.linspace(0, total - 1, frame_count, dtype=int)
    return [video[i] for i in idx]


def _pick_device(device_choice: str) -> str:
    """é€‰æ‹©è®¾å¤‡"""
    if device_choice == "auto":
        if torch.cuda.is_available():
            return "cuda"
        if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
            return "mps"
        return "cpu"
    if device_choice.startswith("cuda") and torch.cuda.is_available():
        return "cuda"
    if device_choice == "mps" and getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def _download_single_file(repo_ids: list[str], filename: str, target_path: Path):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    if target_path.exists():
        print(f"[QwenVL] Using cached file: {target_path}")
        return

    target_path.parent.mkdir(parents=True, exist_ok=True)

    last_exc: Exception | None = None
    for repo_id in repo_ids:
        print(f"[QwenVL] Downloading {filename} from {repo_id} -> {target_path}")
        try:
            downloaded = hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                repo_type="model",
                local_dir=str(target_path.parent),
                local_dir_use_symlinks=False,
            )
            downloaded_path = Path(downloaded)
            if downloaded_path.exists() and downloaded_path.resolve() != target_path.resolve():
                downloaded_path.replace(target_path)
            if target_path.exists():
                print(f"[QwenVL] Download complete: {target_path}")
            break
        except Exception as exc:
            last_exc = exc
            print(f"[QwenVL] hf_hub_download failed from {repo_id}: {exc}")
    else:
        raise FileNotFoundError(f"[QwenVL] Download failed for {filename}: {last_exc}")

    if not target_path.exists():
        raise FileNotFoundError(f"[QwenVL] File not found after download: {target_path}")


def _resolve_model_entry(model_name: str) -> GGUFVLResolved:
    """è§£ææ¨¡å‹æ¡ç›®"""
    all_models = GGUF_VL_CATALOG.get("models") or {}
    entry = all_models.get(model_name) or {}
    if not entry:
        wanted = _model_name_to_filename_candidates(model_name)
        for candidate in all_models.values():
            filename = candidate.get("filename")
            if filename and Path(filename).name in wanted:
                entry = candidate
                break

    repo_id = entry.get("repo_id")
    alt_repo_ids = entry.get("alt_repo_ids") or []

    author = entry.get("author") or entry.get("publisher")
    repo_dirname = entry.get("repo_dirname") or (repo_id.split("/")[-1] if isinstance(repo_id, str) and "/" in repo_id else model_name)

    model_filename = entry.get("filename")
    mmproj_filename = entry.get("mmproj_filename")

    if not model_filename:
        raise ValueError(f"[QwenVL] gguf_vl_models.json entry missing 'filename' for: {model_name}")

    def _int(name: str, default: int) -> int:
        value = entry.get(name, default)
        try:
            return int(value)
        except Exception:
            return default

    return GGUFVLResolved(
        display_name=model_name,
        repo_id=repo_id,
        alt_repo_ids=[str(x) for x in alt_repo_ids if x],
        author=str(author) if author else None,
        repo_dirname=_safe_dirname(str(repo_dirname)),
        model_filename=str(model_filename),
        mmproj_filename=str(mmproj_filename) if mmproj_filename else None,
        context_length=_int("context_length", 8192),
        image_max_tokens=_int("image_max_tokens", 4096),
        n_batch=_int("n_batch", 512),
        gpu_layers=_int("gpu_layers", -1),
        top_k=_int("top_k", 0),
        pool_size=_int("pool_size", 4194304),
    )


def _get_local_gguf_files():
    """è·å–æœ¬åœ°GGUFæ–‡ä»¶åˆ—è¡¨"""
    base_dir = _resolve_base_dir(GGUF_VL_CATALOG.get("base_dir") or "llm/GGUF")
    gguf_files = []
    
    if base_dir.exists():
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰.ggufæ–‡ä»¶
        for file_path in base_dir.rglob("*.gguf"):
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¾¿äºæ˜¾ç¤º
            try:
                rel_path = file_path.relative_to(base_dir)
                display_name = f"æœ¬åœ°: {rel_path}"
                gguf_files.append((str(file_path), display_name))
            except ValueError:
                gguf_files.append((str(file_path), f"æœ¬åœ°: {file_path.name}"))
    
    # æŒ‰æ–‡ä»¶åæ’åº
    gguf_files.sort(key=lambda x: x[1])
    return gguf_files


def _get_local_mmproj_files():
    """è·å–æœ¬åœ°mmprojæ–‡ä»¶åˆ—è¡¨"""
    base_dir = _resolve_base_dir(GGUF_VL_CATALOG.get("base_dir") or "llm/GGUF")
    mmproj_files = [("æ— ", "æ—  mmproj æ–‡ä»¶")]
    
    if base_dir.exists():
        # æŸ¥æ‰¾æ‰€æœ‰å¸¸è§çš„mmprojæ–‡ä»¶æ‰©å±•å
        mmproj_extensions = ['.mmproj', '.gguf', '.bin', '.safetensors']
        
        for file_path in base_dir.rglob("*"):
            if file_path.suffix.lower() in mmproj_extensions:
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«mmprojç›¸å…³å…³é”®è¯
                filename_lower = file_path.name.lower()
                if any(keyword in filename_lower for keyword in ['mmproj', 'vision', 'clip', 'visual']):
                    try:
                        rel_path = file_path.relative_to(base_dir)
                        display_name = f"æœ¬åœ°: {rel_path}"
                        mmproj_files.append((str(file_path), display_name))
                    except ValueError:
                        mmproj_files.append((str(file_path), f"æœ¬åœ°: {file_path.name}"))
    
    # æŒ‰æ–‡ä»¶åæ’åº
    mmproj_files.sort(key=lambda x: x[1])
    return mmproj_files


class QwenVLGGUFBase:
    """QwenVL GGUFåŸºç¡€ç±» - æ”¯æŒå¤šå›¾è¾“å…¥å’Œæœ¬åœ°æ–‡ä»¶é€‰æ‹©"""
    
    def __init__(self):
        self.llm = None
        self.chat_handler = None
        self.current_signature = None

    def clear(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        self.llm = None
        self.chat_handler = None
        self.current_signature = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def _load_backend(self):
        """åŠ è½½åç«¯åº“"""
        try:
            from llama_cpp import Llama  # noqa: F401
        except Exception as exc:
            raise RuntimeError(
                "[QwenVL] llama_cpp is not available. Install the GGUF vision dependency first. See docs/GGUF_MANUAL_INSTALL.md"
            ) from exc

    def _create_chat_handler(self, handler_cls, mmproj_path, img_max):
        """æ ¹æ®å¤„ç†å™¨ç±»åˆ›å»ºç›¸åº”çš„å¤„ç†å™¨å®ä¾‹"""
        handler_name = handler_cls.__name__
        
        # å®šä¹‰ä¸åŒå¤„ç†å™¨ç±»æ”¯æŒçš„å‚æ•°
        param_templates = {
            "Qwen3VLChatHandler": {
                "clip_model_path": str(mmproj_path),
                "image_max_tokens": img_max,
                "force_reasoning": False,
                "verbose": False,
            },
            "Qwen25VLChatHandler": {
                "clip_model_path": str(mmproj_path),
                "image_max_tokens": img_max,
                "force_reasoning": False,
                "verbose": False,
            },
            "Llava15ChatHandler": {
                "clip_model_path": str(mmproj_path),
                "verbose": False,
                # Llava15ChatHandler ä¸æ”¯æŒ image_max_tokens
            },
            "Llava16ChatHandler": {
                "clip_model_path": str(mmproj_path),
                "verbose": False,
                # Llava16ChatHandler å¯èƒ½ä¹Ÿä¸æ”¯æŒ image_max_tokens
            },
            "LlavaChatHandler": {
                "clip_model_path": str(mmproj_path),
                "verbose": False,
                # é€šç”¨ Llava å¤„ç†å™¨
            },
        }
        
        # æ£€æŸ¥å¤„ç†å™¨ç±»å®é™…æ”¯æŒçš„å‚æ•°
        try:
            sig = inspect.signature(handler_cls.__init__)
            supported_params = list(sig.parameters.keys())
        except Exception:
            supported_params = []
        
        # é€‰æ‹©åŸºç¡€å‚æ•°æ¨¡æ¿
        if handler_name in param_templates:
            kwargs = param_templates[handler_name].copy()
        else:
            kwargs = {
                "clip_model_path": str(mmproj_path),
                "verbose": False,
            }
            print(f"[QwenVL] è­¦å‘Š: {handler_name} ä½¿ç”¨é»˜è®¤å‚æ•°")
        
        # è¿‡æ»¤æ‰å¤„ç†å™¨ä¸æ”¯æŒçš„å‚æ•°
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key in supported_params:
                filtered_kwargs[key] = value
            else:
                print(f"[QwenVL] è·³è¿‡ {handler_name} ä¸æ”¯æŒå‚æ•°: {key}")
        
        print(f"[QwenVL] ä½¿ç”¨ {handler_name}ï¼Œå‚æ•°: {list(filtered_kwargs.keys())}")
        return handler_cls(**filtered_kwargs)

    def _load_model(
        self,
        model_source: str,  # æ¨¡å‹æ¥æºï¼šé…ç½®åç§°æˆ–æœ¬åœ°è·¯å¾„
        mmproj_source: str,  # mmprojæ–‡ä»¶æ¥æº
        device: str,
        ctx: int | None,
        n_batch: int | None,
        gpu_layers: int | None,
        image_max_tokens: int | None,
        top_k: int | None,
        pool_size: int | None,
        is_local_file: bool = False,  # æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    ):
        """åŠ è½½æ¨¡å‹ - æ”¯æŒé…ç½®æ¨¡å‹å’Œæœ¬åœ°æ–‡ä»¶"""
        self._load_backend()

        # åˆ¤æ–­æ¨¡å‹æ¥æºç±»å‹
        if is_local_file:
            # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            model_path = Path(model_source)
            mmproj_path = Path(mmproj_source) if mmproj_source and mmproj_source != "æ— " else None
            
            if not model_path.exists():
                raise FileNotFoundError(f"[QwenVL] æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
            if mmproj_path and not mmproj_path.exists():
                print(f"[QwenVL] è­¦å‘Š: mmprojæ–‡ä»¶ä¸å­˜åœ¨: {mmproj_path}ï¼Œå°†ä¸ä½¿ç”¨è§†è§‰åŠŸèƒ½")
                mmproj_path = None
                
            # ä½¿ç”¨é»˜è®¤é…ç½®å€¼
            resolved = GGUFVLResolved(
                display_name=model_path.name,
                repo_id=None,
                alt_repo_ids=[],
                author=None,
                repo_dirname=model_path.parent.name,
                model_filename=model_path.name,
                mmproj_filename=mmproj_path.name if mmproj_path else None,
                context_length=8192,
                image_max_tokens=4096,
                n_batch=512,
                gpu_layers=-1,
                top_k=0,
                pool_size=4194304,
            )
        else:
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
            resolved = _resolve_model_entry(model_source)
            base_dir = _resolve_base_dir(GGUF_VL_CATALOG.get("base_dir") or "llm/GGUF")

            author_dir = _safe_dirname(resolved.author or "")
            repo_dir = _safe_dirname(resolved.repo_dirname)
            target_dir = base_dir / author_dir / repo_dir

            model_path = target_dir / Path(resolved.model_filename).name
            mmproj_path = target_dir / Path(resolved.mmproj_filename).name if resolved.mmproj_filename else None

            repo_ids: list[str] = []
            if resolved.repo_id:
                repo_ids.append(resolved.repo_id)
            repo_ids.extend(resolved.alt_repo_ids)

            if not model_path.exists():
                if not repo_ids:
                    raise FileNotFoundError(f"[QwenVL] GGUF model not found locally and no repo_id provided: {model_path}")
                _download_single_file(repo_ids, resolved.model_filename, model_path)

            if mmproj_path is not None and not mmproj_path.exists():
                if not repo_ids:
                    raise FileNotFoundError(f"[QwenVL] mmproj not found locally and no repo_id provided: {mmproj_path}")
                _download_single_file(repo_ids, resolved.mmproj_filename, mmproj_path)

        device_kind = _pick_device(device)

        n_ctx = int(ctx) if ctx is not None else resolved.context_length
        n_batch_val = int(n_batch) if n_batch is not None else resolved.n_batch
        top_k_val = int(top_k) if top_k is not None else resolved.top_k
        pool_size_val = int(pool_size) if pool_size is not None else resolved.pool_size

        if device_kind == "cuda":
            n_gpu_layers = int(gpu_layers) if gpu_layers is not None else resolved.gpu_layers
        else:
            n_gpu_layers = 0

        img_max = int(image_max_tokens) if image_max_tokens is not None else resolved.image_max_tokens

        has_mmproj = mmproj_path is not None and mmproj_path.exists()

        signature = (
            str(model_path),
            str(mmproj_path) if has_mmproj else "",
            n_ctx,
            n_batch_val,
            n_gpu_layers,
            img_max,
            top_k_val,
            pool_size_val,
        )
        if self.llm is not None and self.current_signature == signature:
            return

        self.clear()

        from llama_cpp import Llama

        self.chat_handler = None
        if has_mmproj:
            handler_classes_to_try = [
                ("Qwen3VLChatHandler", "from llama_cpp.llama_chat_format import Qwen3VLChatHandler"),
                ("Qwen25VLChatHandler", "from llama_cpp.llama_chat_format import Qwen25VLChatHandler"),
                ("Llava15ChatHandler", "from llama_cpp.llama_chat_format import Llava15ChatHandler"),
                ("Llava16ChatHandler", "from llama_cpp.llama_chat_format import Llava16ChatHandler"),
                ("LlavaChatHandler", "from llama_cpp.llama_chat_format import LlavaChatHandler"),
            ]
            
            handler_cls = None
            handler_name = ""
            
            for hname, import_stmt in handler_classes_to_try:
                try:
                    # åŠ¨æ€å¯¼å…¥å¤„ç†å™¨ç±»
                    exec(import_stmt)
                    handler_cls = eval(hname)
                    handler_name = hname
                    print(f"[QwenVL] æ‰¾åˆ°å¤„ç†å™¨: {handler_name}")
                    break
                except ImportError:
                    continue
                except Exception as e:
                    print(f"[QwenVL] å¯¼å…¥ {hname} å¤±è´¥: {e}")
                    continue
            
            if handler_cls is None:
                # å°è¯•æœ€åçš„é€šç”¨æ–¹æ³•
                try:
                    # å°è¯•å¯¼å…¥é€šç”¨çš„å¤„ç†å™¨
                    from llama_cpp.llama_chat_format import LlavaChatHandler
                    handler_cls = LlavaChatHandler
                    handler_name = "LlavaChatHandler"
                except ImportError:
                    raise RuntimeError(
                        "[QwenVL] Missing vision chat handler in llama_cpp. Install the correct fork/wheel. See docs/GGUF_MANUAL_INSTALL.md"
                    )
            
            try:
                self.chat_handler = self._create_chat_handler(handler_cls, mmproj_path, img_max)
            except Exception as e:
                print(f"[QwenVL] åˆ›å»º {handler_name} å¤„ç†å™¨å¤±è´¥: {e}")
                # å°è¯•ä½¿ç”¨æœ€ç®€å•çš„å‚æ•°
                try:
                    print(f"[QwenVL] å°è¯•ä½¿ç”¨æœ€å°å‚æ•°é›†åˆ›å»ºå¤„ç†å™¨")
                    self.chat_handler = handler_cls(clip_model_path=str(mmproj_path), verbose=False)
                except Exception as e2:
                    print(f"[QwenVL] æœ€å°å‚æ•°ä¹Ÿå¤±è´¥: {e2}")
                    print(f"[QwenVL] è­¦å‘Š: æ— æ³•åˆ›å»ºè§†è§‰å¤„ç†å™¨ï¼Œå›¾åƒåŠŸèƒ½å°†ä¸å¯ç”¨")
                    self.chat_handler = None
                    has_mmproj = False

        llm_kwargs = {
            "model_path": str(model_path),
            "n_ctx": n_ctx,
            "n_gpu_layers": n_gpu_layers,
            "n_batch": n_batch_val,
            "swa_full": True,
            "verbose": False,
            "pool_size": pool_size_val,
            "top_k": top_k_val,
        }
        
        # å°è¯•æ·»åŠ  chat_handler
        if has_mmproj and self.chat_handler is not None:
            try:
                llm_kwargs["chat_handler"] = self.chat_handler
                llm_kwargs["image_min_tokens"] = 1024
                # åªæœ‰ Qwen å¤„ç†å™¨æ”¯æŒ image_max_tokens
                if handler_name in ["Qwen3VLChatHandler", "Qwen25VLChatHandler"]:
                    llm_kwargs["image_max_tokens"] = img_max
                print(f"[QwenVL] å·²æ·»åŠ  {handler_name} ä½œä¸º chat_handler")
            except Exception as e:
                print(f"[QwenVL] è­¦å‘Š: æ·»åŠ  chat_handler å¤±è´¥: {e}")
                print(f"[QwenVL] å›¾åƒåŠŸèƒ½å¯èƒ½å—é™")

        print(f"[QwenVL] Loading GGUF: {model_path.name} (device={device_kind}, gpu_layers={n_gpu_layers}, ctx={n_ctx})")
        
        # è¿‡æ»¤æ‰ Llama ä¸æ”¯æŒçš„å‚æ•°
        llm_kwargs_filtered = _filter_kwargs_for_callable(getattr(Llama, "__init__", Llama), llm_kwargs)
        
        # æ£€æŸ¥ chat_handler æ˜¯å¦è¢«æ¥å—
        if has_mmproj and self.chat_handler is not None and "chat_handler" not in llm_kwargs_filtered:
            print(
                "[QwenVL] è­¦å‘Š: å½“å‰ llama_cpp ç‰ˆæœ¬ä¸æ”¯æŒ chat_handler å‚æ•°ã€‚"
                "è¿™å¯èƒ½æ˜¯å› ä¸ºæ‚¨ä½¿ç”¨çš„æ˜¯æ—§ç‰ˆæœ¬æˆ–ä¸æ”¯æŒå¤šæ¨¡æ€çš„æ„å»ºã€‚"
                "è¯·æ›´æ–°åˆ°æ”¯æŒå¤šæ¨¡æ€çš„ llama-cpp-python ç‰ˆæœ¬ã€‚"
            )
            # ç§»é™¤ chat_handler ç›¸å…³å‚æ•°
            llm_kwargs_filtered.pop("chat_handler", None)
            llm_kwargs_filtered.pop("image_min_tokens", None)
            llm_kwargs_filtered.pop("image_max_tokens", None)
            
        if device_kind == "cuda" and n_gpu_layers == 0:
            print("[QwenVL] è­¦å‘Š: device=cuda ä½† n_gpu_layers=0ï¼Œæ¨¡å‹å°†åœ¨ CPU ä¸Šè¿è¡Œ")
            
        try:
            self.llm = Llama(**llm_kwargs_filtered)
            self.current_signature = signature
            print(f"[QwenVL] æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"[QwenVL] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å°è¯•å»æ‰å¯èƒ½çš„é¢å¤–å‚æ•°
            minimal_kwargs = {
                "model_path": str(model_path),
                "n_ctx": n_ctx,
                "n_gpu_layers": n_gpu_layers,
                "n_batch": n_batch_val,
                "verbose": False,
            }
            try:
                self.llm = Llama(**minimal_kwargs)
                self.current_signature = signature
                print(f"[QwenVL] ä½¿ç”¨æœ€å°å‚æ•°é›†åŠ è½½æ¨¡å‹æˆåŠŸ")
            except Exception as e2:
                raise RuntimeError(f"[QwenVL] æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: {e2}")

    def _invoke(
        self,
        system_prompt: str,
        user_prompt: str,
        images_b64: list[str],  # æ‰€æœ‰å›¾åƒï¼ŒæŒ‰è¾“å…¥é¡ºåº
        max_tokens: int,
        temperature: float,
        top_p: float,
        repetition_penalty: float,
        seed: int,
    ) -> str:
        """è°ƒç”¨æ¨¡å‹ç”Ÿæˆ - æ”¯æŒæŒ‰é¡ºåºå¤„ç†å›¾åƒ"""
        
        # æ„å»ºæ¶ˆæ¯ï¼Œå›¾åƒæŒ‰è¾“å…¥é¡ºåºé™„åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆä»…æ–‡æœ¬ï¼‰
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«ç”¨æˆ·æ–‡æœ¬å’Œæ‰€æœ‰å›¾åƒï¼‰
        user_content = []
        
        # æ·»åŠ ç”¨æˆ·æ–‡æœ¬æç¤º
        if user_prompt:
            user_content.append({"type": "text", "text": user_prompt})
        
        # æŒ‰è¾“å…¥é¡ºåºæ·»åŠ æ‰€æœ‰å›¾åƒ
        for i, img in enumerate(images_b64):
            if img:
                user_content.append({"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img}"}})
        
        if user_content:
            messages.append({"role": "user", "content": user_content})
        
        print(f"[QwenVL] æ€»å…±è¾“å…¥ {len(images_b64)} å¼ å›¾åƒï¼Œå°†æŒ‰è¾“å…¥é¡ºåºå¤„ç†")
        
        start = time.perf_counter()
        try:
            result = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                repeat_penalty=float(repetition_penalty),
                seed=int(seed),
                stop=["<|im_end|>", "<|im_start|>"],
            )
        except Exception as e:
            print(f"[QwenVL] ç”Ÿæˆå¤±è´¥: {e}")
            # å°è¯•ç®€åŒ–è°ƒç”¨
            try:
                print(f"[QwenVL] å°è¯•ç®€åŒ–ç”Ÿæˆè°ƒç”¨")
                result = self.llm.create_chat_completion(
                    messages=messages,
                    max_tokens=int(max_tokens),
                    temperature=float(temperature),
                    top_p=float(top_p),
                )
            except Exception as e2:
                print(f"[QwenVL] ç®€åŒ–è°ƒç”¨ä¹Ÿå¤±è´¥: {e2}")
                return f"[é”™è¯¯] ç”Ÿæˆå¤±è´¥: {e2}"
                
        elapsed = max(time.perf_counter() - start, 1e-6)

        usage = result.get("usage") or {}
        prompt_tokens = usage.get("prompt_tokens")
        completion_tokens = usage.get("completion_tokens")
        if isinstance(completion_tokens, int) and completion_tokens > 0:
            tok_s = completion_tokens / elapsed
            if isinstance(prompt_tokens, int) and prompt_tokens >= 0:
                print(
                    f"[QwenVL] Tokens: prompt={prompt_tokens}, completion={completion_tokens}, "
                    f"time={elapsed:.2f}s, speed={tok_s:.2f} tok/s"
                )
            else:
                print(f"[QwenVL] Tokens: completion={completion_tokens}, time={elapsed:.2f}s, speed={tok_s:.2f} tok/s")

        content = (result.get("choices") or [{}])[0].get("message", {}).get("content", "")
        cleaned = clean_model_output(str(content or ""), OutputCleanConfig(mode="text"))
        return cleaned.strip()

    def run(
        self,
        model_source: str,  # æ¨¡å‹æ¥æºï¼šé…ç½®åç§°æˆ–æœ¬åœ°è·¯å¾„
        mmproj_source: str,  # mmprojæ–‡ä»¶æ¥æº
        use_local_files: bool,  # æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        system_prompt: str,    # ç³»ç»Ÿè§’è‰²å®šä¹‰æç¤ºè¯
        user_prompt: str,      # ç”¨æˆ·è¾“å…¥æç¤ºè¯
        images: list,          # æ‰€æœ‰å›¾åƒåˆ—è¡¨ï¼ŒæŒ‰è¾“å…¥é¡ºåº
        video,
        frame_count: int,
        max_tokens: int,
        temperature: float,
        top_p: float,
        repetition_penalty: float,
        seed: int,
        keep_model_loaded: bool,
        device: str,
        ctx: int | None,
        n_batch: int | None,
        gpu_layers: int | None,
        image_max_tokens: int | None,
        top_k: int | None,
        pool_size: int | None,
    ):
        """è¿è¡Œæ¨¡å‹ç”Ÿæˆ"""
        torch.manual_seed(int(seed))

        # å¤„ç†æ‰€æœ‰å›¾åƒï¼ŒæŒ‰è¾“å…¥é¡ºåº
        images_b64: list[str] = []
        if images:
            for i, image_tensor in enumerate(images):
                if image_tensor is not None:
                    img = _tensor_to_base64_png(image_tensor)
                    if img:
                        images_b64.append(img)
                        print(f"[QwenVL] å›¾åƒ{i+1}: å·²è½¬æ¢")
        
        # å¤„ç†è§†é¢‘è¾“å…¥ï¼ˆè§†é¢‘é€šå¸¸ä½œä¸ºç”¨æˆ·è¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼‰
        if video is not None:
            for frame in _sample_video_frames(video, int(frame_count)):
                img = _tensor_to_base64_png(frame)
                if img:
                    images_b64.append(img)

        try:
            self._load_model(
                model_source=model_source,
                mmproj_source=mmproj_source,
                device=device,
                ctx=ctx,
                n_batch=n_batch,
                gpu_layers=gpu_layers,
                image_max_tokens=image_max_tokens,
                top_k=top_k,
                pool_size=pool_size,
                is_local_file=use_local_files,
            )
            
            total_images = len(images_b64)
            if total_images > 0 and self.chat_handler is None:
                print("[QwenVL] è­¦å‘Š: æä¾›äº†å›¾åƒä½†æ¨¡å‹æ²¡æœ‰è§†è§‰å¤„ç†å™¨ï¼Œå›¾åƒå°†è¢«å¿½ç•¥")
            
            # æ‰“å°å›¾åƒä¿¡æ¯
            if self.chat_handler is not None and total_images > 0:
                print(f"[QwenVL] æ€»å…±è¾“å…¥ {total_images} å¼ å›¾åƒï¼Œå°†æŒ‰è¾“å…¥é¡ºåºå¤„ç†")
            
            text = self._invoke(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                images_b64=images_b64 if self.chat_handler is not None else [],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                seed=seed,
            )
            return (text,)
        except Exception as e:
            print(f"[QwenVL] è¿è¡Œå¤±è´¥: {e}")
            return (f"[é”™è¯¯] {str(e)}",)
        finally:
            if not keep_model_loaded:
                self.clear()


class AILab_QwenVL_GGUF(QwenVLGGUFBase):
    """åŸºç¡€ç‰ˆGGUFèŠ‚ç‚¹ - é»˜è®¤ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œæ”¯æŒå¤šå›¾åˆ†æ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        all_models = GGUF_VL_CATALOG.get("models") or {}
        model_keys = sorted([key for key, entry in all_models.items() if (entry or {}).get("mmproj_filename")]) or ["(edit gguf_models.json)"]
        default_model = model_keys[0] if model_keys else ""

        # è·å–æœ¬åœ°æ–‡ä»¶
        local_gguf_files = _get_local_gguf_files()
        local_mmproj_files = _get_local_mmproj_files()
        
        # è®¾ç½®é»˜è®¤å€¼
        default_model_file = "æ— "
        default_mmproj_file = "æ— "
        
        if local_gguf_files:
            default_model_file = local_gguf_files[0][1]  # ç¬¬ä¸€ä¸ªæœ¬åœ°æ–‡ä»¶
        
        if len(local_mmproj_files) > 1:
            default_mmproj_file = local_mmproj_files[1][1]  # è·³è¿‡ç¬¬ä¸€ä¸ª"æ— "é€‰é¡¹
        
        # å¤šå›¾åˆ†æä¸“ç”¨æç¤ºè¯
        multi_image_prompts = [
            "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡",
            "åˆ†æå›¾ç‰‡çš„è‰ºæœ¯é£æ ¼",
            "æè¿°å›¾ç‰‡ä¸­çš„äººç‰©å’Œåœºæ™¯",
            "æå–å›¾ç‰‡çš„å…³é”®ä¿¡æ¯",
            "ä¸ºå›¾ç‰‡åˆ›ä½œä¸€ä¸ªæ•…äº‹",
            "åˆ†æå›¾ç‰‡çš„è‰²å½©å’Œæ„å›¾",
            "æè¿°å›¾ç‰‡ä¸­çš„ç‰©ä½“å’Œå…³ç³»",
            "ä¸ºå›¾ç‰‡ç”Ÿæˆè¯¦ç»†çš„æè¿°"
        ]

        return {
            "required": {
                # é»˜è®¤å¯ç”¨æœ¬åœ°æ–‡ä»¶
                "ä½¿ç”¨æœ¬åœ°æ–‡ä»¶": ("BOOLEAN", {"default": True, "tooltip": "å¯ç”¨åä½¿ç”¨æœ¬åœ°GGUFæ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹"}),
                "æ¨¡å‹é€‰æ‹©æ–¹å¼": (["ä»é…ç½®é€‰æ‹©", "æœ¬åœ°æ–‡ä»¶"], {"default": "æœ¬åœ°æ–‡ä»¶", "tooltip": "é€‰æ‹©æ¨¡å‹åŠ è½½æ–¹å¼"}),
                "model_name": (model_keys, {"default": default_model, "tooltip": "ä»é…ç½®ä¸­é€‰æ‹©æ¨¡å‹"}),
                "æœ¬åœ°æ¨¡å‹æ–‡ä»¶": (["æ— "] + [display for _, display in local_gguf_files], {"default": "æ— ", "tooltip": "é€‰æ‹©æœ¬åœ°GGUFæ–‡ä»¶"}),
                "æœ¬åœ°mmprojæ–‡ä»¶": (["æ— "] + [display for _, display in local_mmproj_files], {"default": "æ— ", "tooltip": "é€‰æ‹©æœ¬åœ°mmprojæ–‡ä»¶ï¼ˆè§†è§‰æ¨¡å‹éœ€è¦ï¼‰"}),
                
                # æç¤ºè¯é…ç½®
                "åˆ†ææ¨¡å¼": (["å•å›¾æè¿°", "å¤šå›¾å¯¹æ¯”", "å¤šå›¾åˆ†æ"], {"default": "å•å›¾æè¿°", "tooltip": "é€‰æ‹©åˆ†ææ¨¡å¼"}),
                "é¢„è®¾æç¤ºè¯": (multi_image_prompts, {"default": multi_image_prompts[0], "tooltip": "é€‰æ‹©é¢„è®¾çš„å¤šå›¾åˆ†ææç¤ºè¯"}),
                "è‡ªå®šä¹‰æç¤ºè¯": ("STRING", {"default": "", "multiline": True, "placeholder": "è¾“å…¥è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰"}),
                "ç³»ç»Ÿè§’è‰²å®šä¹‰": ("STRING", {"default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ç†è§£å’Œæè¿°å›¾åƒå†…å®¹ã€‚", "multiline": True, "placeholder": "å®šä¹‰AIçš„ç³»ç»Ÿè§’è‰²"}),
                
                # åŸºæœ¬å‚æ•°
                "max_tokens": ("INT", {"default": 1024, "min": 256, "max": 4096, "tooltip": "æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°"}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 1.5, "step": 0.1, "tooltip": "æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§"}),
                "keep_model_loaded": ("BOOLEAN", {"default": True, "tooltip": "ä¿æŒæ¨¡å‹åŠ è½½ä»¥åŠ é€Ÿåç»­æ¨ç†"}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 2**32 - 1, "tooltip": "éšæœºç§å­ï¼Œ-1ä¸ºéšæœº"}),
            },
            "optional": {
                # å›¾åƒè¾“å…¥ï¼ˆæ”¯æŒå¤šå›¾ï¼‰
                "å›¾åƒ_1": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 1"}),
                "å›¾åƒ_2": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 2"}),
                "å›¾åƒ_3": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 3"}),
                "å›¾åƒ_4": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 4"}),
                "å›¾åƒ_5": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 5"}),
                "å›¾åƒ_6": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 6"}),
                
                "video": ("IMAGE", {"tooltip": "è§†é¢‘è¾“å…¥ï¼ˆå¯é€‰ï¼‰"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("åˆ†æç»“æœ",)
    FUNCTION = "process"
    CATEGORY = "ğŸ§ªAILab/QwenVL"

    def process(
        self,
        ä½¿ç”¨æœ¬åœ°æ–‡ä»¶=True,
        æ¨¡å‹é€‰æ‹©æ–¹å¼="æœ¬åœ°æ–‡ä»¶",
        model_name="æ— ",
        æœ¬åœ°æ¨¡å‹æ–‡ä»¶="æ— ",
        æœ¬åœ°mmprojæ–‡ä»¶="æ— ",
        åˆ†ææ¨¡å¼="å•å›¾æè¿°",
        é¢„è®¾æç¤ºè¯="è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡",
        è‡ªå®šä¹‰æç¤ºè¯="",
        ç³»ç»Ÿè§’è‰²å®šä¹‰="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ç†è§£å’Œæè¿°å›¾åƒå†…å®¹ã€‚",
        max_tokens=1024,
        temperature=0.7,
        keep_model_loaded=True,
        seed=-1,
        å›¾åƒ_1=None,
        å›¾åƒ_2=None,
        å›¾åƒ_3=None,
        å›¾åƒ_4=None,
        å›¾åƒ_5=None,
        å›¾åƒ_6=None,
        video=None,
    ):
        # æ”¶é›†æ‰€æœ‰å›¾åƒï¼ŒæŒ‰è¾“å…¥é¡ºåº
        images = [å›¾åƒ_1, å›¾åƒ_2, å›¾åƒ_3, å›¾åƒ_4, å›¾åƒ_5, å›¾åƒ_6]
        images = [img for img in images if img is not None]
        
        # æ ¹æ®åˆ†ææ¨¡å¼è°ƒæ•´æç¤ºè¯
        if åˆ†ææ¨¡å¼ == "å¤šå›¾å¯¹æ¯”":
            if not è‡ªå®šä¹‰æç¤ºè¯.strip():
                base_prompt = "è¯·æ¯”è¾ƒå’Œåˆ†æè¿™äº›å›¾ç‰‡çš„ç›¸ä¼¼ä¹‹å¤„å’Œå·®å¼‚ï¼š"
            else:
                base_prompt = è‡ªå®šä¹‰æç¤ºè¯.strip()
        elif åˆ†ææ¨¡å¼ == "å¤šå›¾åˆ†æ":
            if not è‡ªå®šä¹‰æç¤ºè¯.strip():
                base_prompt = "è¯·ç»¼åˆåˆ†æè¿™äº›å›¾ç‰‡ï¼Œæè¿°å®ƒä»¬å…±åŒçš„ä¸»é¢˜å’Œå„è‡ªçš„ç‰¹ç‚¹ï¼š"
            else:
                base_prompt = è‡ªå®šä¹‰æç¤ºè¯.strip()
        else:  # å•å›¾æè¿°
            if not è‡ªå®šä¹‰æç¤ºè¯.strip():
                base_prompt = é¢„è®¾æç¤ºè¯
            else:
                base_prompt = è‡ªå®šä¹‰æç¤ºè¯.strip()
        
        # å¦‚æœæœ‰å¤šä¸ªå›¾åƒï¼Œè‡ªåŠ¨è°ƒæ•´æç¤ºè¯
        if len(images) > 1 and åˆ†ææ¨¡å¼ == "å•å›¾æè¿°":
            base_prompt = f"è¯·æŒ‰é¡ºåºæè¿°è¿™{len(images)}å¼ å›¾ç‰‡ï¼š{base_prompt}"
        
        # æ ¹æ®å›¾åƒæ•°é‡è°ƒæ•´ç³»ç»Ÿè§’è‰²
        if len(images) > 1:
            if "å¤šå›¾" not in ç³»ç»Ÿè§’è‰²å®šä¹‰:
                ç³»ç»Ÿè§’è‰²å®šä¹‰ = f"{ç³»ç»Ÿè§’è‰²å®šä¹‰}ä½ ç‰¹åˆ«æ“…é•¿å¤šå›¾åˆ†æå’Œå¯¹æ¯”ã€‚"
        
        # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        use_local = ä½¿ç”¨æœ¬åœ°æ–‡ä»¶  # é»˜è®¤å°±æ˜¯True
        
        # è·å–å®é™…æ–‡ä»¶è·¯å¾„
        model_source = "æ— "
        mmproj_source = "æ— "
        
        if use_local:
            # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶è·¯å¾„
            local_gguf_files = _get_local_gguf_files()
            for file_path, display_name in local_gguf_files:
                if display_name == æœ¬åœ°æ¨¡å‹æ–‡ä»¶:
                    model_source = file_path
                    break
            
            # æŸ¥æ‰¾mmprojæ–‡ä»¶è·¯å¾„
            if æœ¬åœ°mmprojæ–‡ä»¶ != "æ— ":
                local_mmproj_files = _get_local_mmproj_files()
                for file_path, display_name in local_mmproj_files:
                    if display_name == æœ¬åœ°mmprojæ–‡ä»¶:
                        mmproj_source = file_path
                        break
            else:
                mmproj_source = "æ— "
                
            if model_source == "æ— ":
                raise ValueError("è¯·é€‰æ‹©æœ‰æ•ˆçš„æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
        else:
            raise ValueError("æœ¬èŠ‚ç‚¹å·²é…ç½®ä¸ºé»˜è®¤ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œè¯·å–æ¶ˆå‹¾é€‰'ä½¿ç”¨æœ¬åœ°æ–‡ä»¶'ä»¥ä½¿ç”¨é…ç½®æ¨¡å‹")
        
        print(f"[QwenVL] å¤šå›¾åˆ†ææ¨¡å¼: {åˆ†ææ¨¡å¼}")
        print(f"[QwenVL] è¾“å…¥ {len(images)} å¼ å›¾åƒï¼Œå°†æŒ‰è¾“å…¥é¡ºåºå¤„ç†")
        print(f"[QwenVL] ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {Path(model_source).name}")
        
        # å¦‚æœç§å­ä¸º-1ï¼Œä½¿ç”¨éšæœºç§å­
        effective_seed = seed if seed != -1 else random.randint(1, 2**32 - 1)
        
        return self.run(
            model_source=model_source,
            mmproj_source=mmproj_source,
            use_local_files=use_local,
            system_prompt=ç³»ç»Ÿè§’è‰²å®šä¹‰,
            user_prompt=base_prompt,
            images=images,
            video=video,
            frame_count=8,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.9,
            repetition_penalty=1.1,
            seed=effective_seed,
            keep_model_loaded=keep_model_loaded,
            device="auto",
            ctx=None,
            n_batch=None,
            gpu_layers=None,
            image_max_tokens=None,
            top_k=None,
            pool_size=None,
        )


class AILab_QwenVL_GGUF_Advanced(QwenVLGGUFBase):
    """é«˜çº§ç‰ˆGGUFèŠ‚ç‚¹ - é»˜è®¤ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œæ”¯æŒé«˜çº§å¤šå›¾åˆ†æ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        all_models = GGUF_VL_CATALOG.get("models") or {}
        model_keys = sorted([key for key, entry in all_models.items() if (entry or {}).get("mmproj_filename")]) or ["(edit gguf_models.json)"]
        default_model = model_keys[0] if model_keys else ""
        # è·å–æœ¬åœ°æ–‡ä»¶
        local_gguf_files = _get_local_gguf_files()
        local_mmproj_files = _get_local_mmproj_files()
        
        # è®¾ç½®é»˜è®¤å€¼
        default_model_file = "æ— "
        default_mmproj_file = "æ— "
        
        if local_gguf_files:
            default_model_file = local_gguf_files[0][1]
        
        if len(local_mmproj_files) > 1:
            default_mmproj_file = local_mmproj_files[1][1]
        
        num_gpus = torch.cuda.device_count()
        gpu_list = [f"cuda:{i}" for i in range(num_gpus)]
        device_options = ["auto", "cpu", "mps"] + gpu_list
        
        # é«˜çº§åˆ†ææ¨¡å¼
        advanced_modes = [
            "å•å›¾è¯¦ç»†æè¿°",
            "å¤šå›¾å¯¹æ¯”åˆ†æ", 
            "å¤šå›¾æ•…äº‹åˆ›ä½œ",
            "å¤šå›¾ä¸»é¢˜æå–",
            "è‰ºæœ¯é£æ ¼åˆ†æ",
            "æŠ€æœ¯ç»†èŠ‚åˆ†æ",
            "æƒ…æ„Ÿæ°›å›´åˆ†æ",
            "åˆ›æ„çµæ„Ÿç”Ÿæˆ"
        ]

        return {
            "required": {
                # é»˜è®¤å¯ç”¨æœ¬åœ°æ–‡ä»¶
                "ä½¿ç”¨æœ¬åœ°æ–‡ä»¶": ("BOOLEAN", {"default": True, "tooltip": "å¯ç”¨åä½¿ç”¨æœ¬åœ°GGUFæ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹"}),
                "æ¨¡å‹é€‰æ‹©æ–¹å¼": (["ä»é…ç½®é€‰æ‹©", "æœ¬åœ°æ–‡ä»¶"], {"default": "æœ¬åœ°æ–‡ä»¶", "tooltip": "é€‰æ‹©æ¨¡å‹åŠ è½½æ–¹å¼"}),
                "model_name": (model_keys, {"default": default_model, "tooltip": "ä»é…ç½®ä¸­é€‰æ‹©æ¨¡å‹"}),
                "æœ¬åœ°æ¨¡å‹æ–‡ä»¶": (["æ— "] + [display for _, display in local_gguf_files], {"default": "æ— ", "tooltip": "é€‰æ‹©æœ¬åœ°GGUFæ–‡ä»¶"}),
                "æœ¬åœ°mmprojæ–‡ä»¶": (["æ— "] + [display for _, display in local_mmproj_files], {"default": "æ— ", "tooltip": "é€‰æ‹©æœ¬åœ°mmprojæ–‡ä»¶ï¼ˆè§†è§‰æ¨¡å‹éœ€è¦ï¼‰"}),
                
                # é«˜çº§åˆ†æé…ç½®
                "åˆ†ææ¨¡å¼": (advanced_modes, {"default": advanced_modes[0], "tooltip": "é€‰æ‹©é«˜çº§åˆ†ææ¨¡å¼"}),
                "è‡ªå®šä¹‰æç¤ºè¯": ("STRING", {"default": "", "multiline": True, "placeholder": "è¾“å…¥è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰"}),
                "ç³»ç»Ÿè§’è‰²å®šä¹‰": ("STRING", {"default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·æœ‰æ·±åšçš„è‰ºæœ¯å’ŒæŠ€æœ¯åˆ†æèƒ½åŠ›ã€‚", "multiline": True, "placeholder": "å®šä¹‰AIçš„ç³»ç»Ÿè§’è‰²"}),
                
                # é«˜çº§å‚æ•°
                "device": (device_options, {"default": "auto", "tooltip": "é€‰æ‹©è®¡ç®—è®¾å¤‡"}),
                "max_tokens": ("INT", {"default": 2048, "min": 512, "max": 8192, "tooltip": "æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°"}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 1.5, "step": 0.1, "tooltip": "æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§"}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.5, "max": 1.0, "step": 0.01, "tooltip": "æ ¸é‡‡æ ·å‚æ•°"}),
                "repetition_penalty": ("FLOAT", {"default": 1.1, "min": 1.0, "max": 2.0, "step": 0.1, "tooltip": "é‡å¤æƒ©ç½šå‚æ•°"}),
                "ctx": ("INT", {"default": 8192, "min": 2048, "max": 32768, "step": 1024, "tooltip": "ä¸Šä¸‹æ–‡é•¿åº¦"}),
                "gpu_layers": ("INT", {"default": -1, "min": -1, "max": 100, "tooltip": "GPUå±‚æ•°ï¼Œ-1ä¸ºè‡ªåŠ¨"}),
                "keep_model_loaded": ("BOOLEAN", {"default": True, "tooltip": "ä¿æŒæ¨¡å‹åŠ è½½ä»¥åŠ é€Ÿåç»­æ¨ç†"}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 2**32 - 1, "tooltip": "éšæœºç§å­ï¼Œ-1ä¸ºéšæœº"}),
            },
            "optional": {
                # æ”¯æŒæ›´å¤šå›¾åƒè¾“å…¥
                "å›¾åƒ_1": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 1"}),
                "å›¾åƒ_2": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 2"}),
                "å›¾åƒ_3": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 3"}),
                "å›¾åƒ_4": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 4"}),
                "å›¾åƒ_5": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 5"}),
                "å›¾åƒ_6": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 6"}),
                "å›¾åƒ_7": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 7"}),
                "å›¾åƒ_8": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 8"}),
                "å›¾åƒ_9": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 9"}),
                "å›¾åƒ_10": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 10"}),
                
                "video": ("IMAGE", {"tooltip": "è§†é¢‘è¾“å…¥ï¼ˆå¯é€‰ï¼‰"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("é«˜çº§åˆ†æç»“æœ",)
    FUNCTION = "process"
    CATEGORY = "ğŸ§ªAILab/QwenVL"

    def process(
        self,
        ä½¿ç”¨æœ¬åœ°æ–‡ä»¶=True,
        æ¨¡å‹é€‰æ‹©æ–¹å¼="æœ¬åœ°æ–‡ä»¶",
        model_name="æ— ",
        æœ¬åœ°æ¨¡å‹æ–‡ä»¶="æ— ",
        æœ¬åœ°mmprojæ–‡ä»¶="æ— ",
        åˆ†ææ¨¡å¼="å•å›¾è¯¦ç»†æè¿°",
        è‡ªå®šä¹‰æç¤ºè¯="",
        ç³»ç»Ÿè§’è‰²å®šä¹‰="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·æœ‰æ·±åšçš„è‰ºæœ¯å’ŒæŠ€æœ¯åˆ†æèƒ½åŠ›ã€‚",
        device="auto",
        max_tokens=2048,
        temperature=0.8,
        top_p=0.95,
        repetition_penalty=1.1,
        ctx=8192,
        gpu_layers=-1,
        keep_model_loaded=True,
        seed=-1,
        å›¾åƒ_1=None,
        å›¾åƒ_2=None,
        å›¾åƒ_3=None,
        å›¾åƒ_4=None,
        å›¾åƒ_5=None,
        å›¾åƒ_6=None,
        å›¾åƒ_7=None,
        å›¾åƒ_8=None,
        å›¾åƒ_9=None,
        å›¾åƒ_10=None,
        video=None,
    ):
        # æ”¶é›†æ‰€æœ‰å›¾åƒï¼ŒæŒ‰è¾“å…¥é¡ºåº
        images = [å›¾åƒ_1, å›¾åƒ_2, å›¾åƒ_3, å›¾åƒ_4, å›¾åƒ_5, å›¾åƒ_6, å›¾åƒ_7, å›¾åƒ_8, å›¾åƒ_9, å›¾åƒ_10]
        images = [img for img in images if img is not None]
        
        # æ ¹æ®åˆ†ææ¨¡å¼ç”Ÿæˆæç¤ºè¯
        mode_prompts = {
            "å•å›¾è¯¦ç»†æè¿°": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“ã€äººç‰©ã€è‰²å½©ã€é£æ ¼ç­‰æ‰€æœ‰è§†è§‰å…ƒç´ ã€‚",
            "å¤šå›¾å¯¹æ¯”åˆ†æ": "è¯·å¯¹æ¯”åˆ†æè¿™äº›å›¾ç‰‡ï¼ŒæŒ‡å‡ºå®ƒä»¬çš„ç›¸ä¼¼ä¹‹å¤„ã€å·®å¼‚ã€å…±åŒä¸»é¢˜å’Œå„è‡ªç‰¹ç‚¹ã€‚",
            "å¤šå›¾æ•…äº‹åˆ›ä½œ": "è¯·æ ¹æ®è¿™äº›å›¾ç‰‡åˆ›ä½œä¸€ä¸ªè¿è´¯çš„æ•…äº‹æˆ–å™äº‹ã€‚",
            "å¤šå›¾ä¸»é¢˜æå–": "è¯·ä»è¿™äº›å›¾ç‰‡ä¸­æå–å…±åŒçš„ä¸»é¢˜ã€æ¦‚å¿µå’Œè§†è§‰å…ƒç´ ã€‚",
            "è‰ºæœ¯é£æ ¼åˆ†æ": "è¯·åˆ†æè¿™äº›å›¾ç‰‡çš„è‰ºæœ¯é£æ ¼ã€ç»˜ç”»æŠ€å·§ã€è‰²å½©è¿ç”¨å’Œæ„å›¾ç‰¹ç‚¹ã€‚",
            "æŠ€æœ¯ç»†èŠ‚åˆ†æ": "è¯·åˆ†æè¿™äº›å›¾ç‰‡çš„æŠ€æœ¯ç»†èŠ‚ï¼ŒåŒ…æ‹¬å…‰çº¿ã€è§’åº¦ã€ç„¦ç‚¹ã€åˆ†è¾¨ç‡ç­‰ã€‚",
            "æƒ…æ„Ÿæ°›å›´åˆ†æ": "è¯·æè¿°è¿™äº›å›¾ç‰‡ä¼ è¾¾çš„æƒ…æ„Ÿæ°›å›´å’Œæƒ…ç»ªæ„Ÿå—ã€‚",
            "åˆ›æ„çµæ„Ÿç”Ÿæˆ": "è¯·åŸºäºè¿™äº›å›¾ç‰‡ç”Ÿæˆåˆ›æ„çµæ„Ÿå’Œè®¾è®¡æ€è·¯ã€‚"
        }
        
        # ç¡®å®šä½¿ç”¨çš„æç¤ºè¯
        if è‡ªå®šä¹‰æç¤ºè¯.strip():
            user_prompt = è‡ªå®šä¹‰æç¤ºè¯.strip()
        else:
            user_prompt = mode_prompts.get(åˆ†ææ¨¡å¼, "è¯·åˆ†æè¿™äº›å›¾ç‰‡ã€‚")
        
        # æ ¹æ®å›¾åƒæ•°é‡è°ƒæ•´æç¤ºè¯
        if len(images) > 1:
            user_prompt = f"å…±æœ‰{len(images)}å¼ å›¾ç‰‡ã€‚{user_prompt}"
        
        # æ ¹æ®åˆ†ææ¨¡å¼è°ƒæ•´ç³»ç»Ÿè§’è‰²
        role_specializations = {
            "è‰ºæœ¯é£æ ¼åˆ†æ": "è‰ºæœ¯è¯„è®ºå®¶",
            "æŠ€æœ¯ç»†èŠ‚åˆ†æ": "æŠ€æœ¯åˆ†æå¸ˆ", 
            "æƒ…æ„Ÿæ°›å›´åˆ†æ": "æƒ…æ„Ÿåˆ†æå¸ˆ",
            "åˆ›æ„çµæ„Ÿç”Ÿæˆ": "åˆ›æ„é¡¾é—®"
        }
        
        specialization = role_specializations.get(åˆ†ææ¨¡å¼, "è§†è§‰åˆ†æä¸“å®¶")
        if specialization not in ç³»ç»Ÿè§’è‰²å®šä¹‰:
            ç³»ç»Ÿè§’è‰²å®šä¹‰ = f"ä½ æ˜¯{specialization}ï¼Œ{ç³»ç»Ÿè§’è‰²å®šä¹‰}"
        
        # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        use_local = ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        
        # è·å–å®é™…æ–‡ä»¶è·¯å¾„
        model_source = "æ— "
        mmproj_source = "æ— "
        
        if use_local:
            # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶è·¯å¾„
            local_gguf_files = _get_local_gguf_files()
            for file_path, display_name in local_gguf_files:
                if display_name == æœ¬åœ°æ¨¡å‹æ–‡ä»¶:
                    model_source = file_path
                    break
            
            # æŸ¥æ‰¾mmprojæ–‡ä»¶è·¯å¾„
            if æœ¬åœ°mmprojæ–‡ä»¶ != "æ— ":
                local_mmproj_files = _get_local_mmproj_files()
                for file_path, display_name in local_mmproj_files:
                    if display_name == æœ¬åœ°mmprojæ–‡ä»¶:
                        mmproj_source = file_path
                        break
            else:
                mmproj_source = "æ— "
                
            if model_source == "æ— ":
                raise ValueError("è¯·é€‰æ‹©æœ‰æ•ˆçš„æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
        else:
            raise ValueError("æœ¬èŠ‚ç‚¹å·²é…ç½®ä¸ºé»˜è®¤ä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
        
        print(f"[QwenVL] é«˜çº§åˆ†ææ¨¡å¼: {åˆ†ææ¨¡å¼}")
        print(f"[QwenVL] è¾“å…¥ {len(images)} å¼ å›¾åƒ")
        print(f"[QwenVL] ä½¿ç”¨è®¾å¤‡: {device}")
        
        # å¦‚æœç§å­ä¸º-1ï¼Œä½¿ç”¨éšæœºç§å­
        effective_seed = seed if seed != -1 else random.randint(1, 2**32 - 1)
        
        return self.run(
            model_source=model_source,
            mmproj_source=mmproj_source,
            use_local_files=use_local,
            system_prompt=ç³»ç»Ÿè§’è‰²å®šä¹‰,
            user_prompt=user_prompt,
            images=images,
            video=video,
            frame_count=12,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            seed=effective_seed,
            keep_model_loaded=keep_model_loaded,
            device=device,
            ctx=ctx,
            n_batch=512,
            gpu_layers=gpu_layers,
            image_max_tokens=4096,
            top_k=40,
            pool_size=4194304,
        )


# æ·»åŠ å¿…è¦çš„import
import random

NODE_CLASS_MAPPINGS = {
    "AILab_QwenVL_GGUF": AILab_QwenVL_GGUF,
    "AILab_QwenVL_GGUF_Advanced": AILab_QwenVL_GGUF_Advanced,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AILab_QwenVL_GGUF": "QwenVL å¤šå›¾åˆ†æ (GGUF)",
    "AILab_QwenVL_GGUF_Advanced": "QwenVL é«˜çº§å¤šå›¾åˆ†æ (GGUF)",
}
