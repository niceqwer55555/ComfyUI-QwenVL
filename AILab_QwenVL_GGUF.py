# ComfyUI-QwenVL (GGUF) - ä¼˜åŒ–ç‰ˆï¼šæ”¯æŒQwen2.5-VLå’Œå¤šå›¾åˆ†æ

import base64
import gc
import io
import inspect
import json
import os
import time
import random
from dataclasses import dataclass
from pathlib import Path

import numpy as np
import torch
from huggingface_hub import hf_hub_download
from PIL import Image

import folder_paths
from AILab_OutputCleaner import OutputCleanConfig, clean_model_output

NODE_DIR = Path(__file__).parent
CONFIG_PATH = NODE_DIR / "hf_models.json"
SYSTEM_PROMPTS_PATH = NODE_DIR / "AILab_System_Prompts.json"
GGUF_CONFIG_PATH = NODE_DIR / "gguf_models.json"


def _load_prompt_config():
    preset_prompts = ["ğŸ–¼ï¸ Detailed Description"]
    system_prompts: dict[str, str] = {}

    try:
        with open(CONFIG_PATH, "r", encoding="utf-8") as fh:
            data = json.load(fh) or {}
        preset_prompts = data.get("_preset_prompts") or preset_prompts
        system_prompts = data.get("_system_prompts") or system_prompts
    except Exception as exc:
        print(f"[QwenVL] Config load failed: {exc}")

    try:
        with open(SYSTEM_PROMPTS_PATH, "r", encoding="utf-8") as fh:
            data = json.load(fh) or {}
        qwenvl_prompts = data.get("qwenvl") or {}
        preset_override = data.get("_preset_prompts") or []
        if isinstance(qwenvl_prompts, dict) and qwenvl_prompts:
            system_prompts = qwenvl_prompts
        if isinstance(preset_override, list) and preset_override:
            preset_prompts = preset_override
    except FileNotFoundError:
        pass
    except Exception as exc:
        print(f"[QwenVL] System prompts load failed: {exc}")

    return preset_prompts, system_prompts


PRESET_PROMPTS, SYSTEM_PROMPTS = _load_prompt_config()


@dataclass(frozen=True)
class GGUFVLResolved:
    display_name: str
    repo_id: str | None
    alt_repo_ids: list[str]
    author: str | None
    repo_dirname: str
    model_filename: str
    mmproj_filename: str | None
    context_length: int
    image_max_tokens: int
    n_batch: int
    gpu_layers: int
    top_k: int
    pool_size: int
    model_family: str = "unknown"


def _resolve_base_dir(base_dir_value: str) -> Path:
    base_dir = Path(base_dir_value)
    if base_dir.is_absolute():
        return base_dir
    return Path(folder_paths.models_dir) / base_dir


def _safe_dirname(value: str) -> str:
    value = (value or "").strip()
    if not value:
        return "unknown"
    return "".join(ch for ch in value if ch.isalnum() or ch in "._- ").strip() or "unknown"


def _model_name_to_filename_candidates(model_name: str) -> set[str]:
    raw = (model_name or "").strip()
    if not raw:
        return set()
    candidates = {raw, f"{raw}.gguf"}
    if " / " in raw:
        tail = raw.split(" / ", 1)[1].strip()
        candidates.update({tail, f"{tail}.gguf"})
    if "/" in raw:
        tail = raw.rsplit("/", 1)[-1].strip()
        candidates.update({tail, f"{tail}.gguf"})
    return candidates


def _detect_model_family_from_filename(filename: str) -> str:
    """ä»æ–‡ä»¶åæ£€æµ‹æ¨¡å‹å®¶æ—"""
    filename_lower = filename.lower()
    
    # ç²¾ç¡®åŒ¹é…
    if "qwen2.5" in filename_lower or "qwen2_5" in filename_lower:
        return "qwen2.5-vl"
    elif "qwen3" in filename_lower:
        return "qwen3-vl"
    elif "qwen2" in filename_lower:
        return "qwen2-vl"
    elif "qwen-vl" in filename_lower:
        return "qwen-vl"
    elif "llava-1.6" in filename_lower:
        return "llava-1.6"
    elif "llava-1.5" in filename_lower:
        return "llava-1.5"
    elif "llava" in filename_lower:
        return "llava"
    elif "bakllava" in filename_lower:
        return "bakllava"
    elif "minicpm" in filename_lower:
        return "minicpm"
    elif "cogvlm" in filename_lower:
        return "cogvlm"
    elif "yi-vl" in filename_lower:
        return "yi-vl"
    elif "deepseek-vl" in filename_lower:
        return "deepseek-vl"
    
    # æ¨¡ç³ŠåŒ¹é…
    if "qwen" in filename_lower and "vl" in filename_lower:
        return "qwen-vl"
    elif "vision" in filename_lower or "visual" in filename_lower:
        return "generic-vl"
    
    return "unknown"


def _detect_model_family_from_path(model_path: Path) -> str:
    """ä»æ¨¡å‹æ–‡ä»¶è·¯å¾„æ›´ç²¾ç¡®åœ°æ£€æµ‹æ¨¡å‹å®¶æ—"""
    if not model_path.exists():
        return "unknown"
    
    filename = model_path.name.lower()
    family = _detect_model_family_from_filename(filename)
    
    if family == "unknown":
        # å°è¯•è¯»å–æ–‡ä»¶å¤´ä¿¡æ¯
        try:
            with open(model_path, 'rb') as f:
                # è¯»å–å‰512å­—èŠ‚
                header = f.read(512)
                header_str = header.decode('ascii', errors='ignore')
                
                # åœ¨æ–‡ä»¶å¤´ä¸­æœç´¢å…³é”®è¯
                if 'qwen' in header_str.lower():
                    if '2.5' in header_str:
                        return "qwen2.5-vl"
                    elif '3' in header_str:
                        return "qwen3-vl"
                    elif '2' in header_str:
                        return "qwen2-vl"
                    else:
                        return "qwen-vl"
                elif 'llava' in header_str.lower():
                    if '1.6' in header_str:
                        return "llava-1.6"
                    elif '1.5' in header_str:
                        return "llava-1.5"
                    else:
                        return "llava"
                elif 'yi' in header_str.lower() and 'vl' in header_str.lower():
                    return "yi-vl"
                elif 'deepseek' in header_str.lower() and 'vl' in header_str.lower():
                    return "deepseek-vl"
        except:
            pass
    
    return family


def _load_gguf_vl_catalog():
    """åŠ è½½GGUFæ¨¡å‹é…ç½®"""
    if not GGUF_CONFIG_PATH.exists():
        return {"base_dir": "LLM/GGUF", "models": {}}
    try:
        with open(GGUF_CONFIG_PATH, "r", encoding="utf-8") as fh:
            data = json.load(fh) or {}
    except Exception as exc:
        print(f"[QwenVL] gguf_models.json load failed: {exc}")
        return {"base_dir": "LLM/GGUF", "models": {}}

    base_dir = data.get("base_dir") or "LLM/GGUF"

    flattened: dict[str, dict] = {}

    repos = data.get("qwenVL_model") or data.get("vl_repos") or data.get("repos") or {}
    seen_display_names: set[str] = set()
    for repo_key, repo in repos.items():
        if not isinstance(repo, dict):
            continue
        author = repo.get("author") or repo.get("publisher")
        repo_name = repo.get("repo_name") or repo.get("repo_name_override") or repo_key
        repo_id = repo.get("repo_id") or (f"{author}/{repo_name}" if author and repo_name else None)
        alt_repo_ids = repo.get("alt_repo_ids") or []

        defaults = repo.get("defaults") or {}
        mmproj_file = repo.get("mmproj_file")
        model_files = repo.get("model_files") or []
        model_family = repo.get("model_family") or "unknown"

        for model_file in model_files:
            display = Path(model_file).name
            if display in seen_display_names:
                display = f"{display} ({repo_key})"
            seen_display_names.add(display)
            flattened[display] = {
                **defaults,
                "author": author,
                "repo_dirname": repo_name,
                "repo_id": repo_id,
                "alt_repo_ids": alt_repo_ids,
                "filename": model_file,
                "mmproj_filename": mmproj_file,
                "model_family": model_family,
            }

    legacy_models = data.get("models") or {}
    for name, entry in legacy_models.items():
        if isinstance(entry, dict):
            if "model_family" not in entry:
                # æ ¹æ®æ–‡ä»¶åçŒœæµ‹æ¨¡å‹å®¶æ—
                filename = entry.get("filename", "")
                entry["model_family"] = _detect_model_family_from_filename(filename)
            flattened[name] = entry

    return {"base_dir": base_dir, "models": flattened}


GGUF_VL_CATALOG = _load_gguf_vl_catalog()


def _filter_kwargs_for_callable(fn, kwargs: dict) -> dict:
    try:
        sig = inspect.signature(fn)
    except Exception:
        return dict(kwargs)

    params = list(sig.parameters.values())
    if any(p.kind == inspect.Parameter.VAR_KEYWORD for p in params):
        return dict(kwargs)

    allowed: set[str] = set()
    for p in params:
        if p.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD, inspect.Parameter.KEYWORD_ONLY):
            allowed.add(p.name)
    return {k: v for k, v in kwargs.items() if k in allowed}


def _tensor_to_base64_png(tensor, resize_to=(448, 448)) -> str | None:
    """å°†å¼ é‡è½¬æ¢ä¸ºbase64 PNGå›¾åƒï¼Œå¯è°ƒæ•´å¤§å°"""
    if tensor is None:
        return None
    if tensor.ndim == 4:
        tensor = tensor[0]
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    array = (tensor * 255).clamp(0, 255).to(torch.uint8).cpu().numpy()
    
    # åˆ›å»ºPILå›¾åƒ
    pil_img = Image.fromarray(array, mode="RGB")
    
    # è°ƒæ•´å¤§å°ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
    if resize_to:
        original_width, original_height = pil_img.size
        target_width, target_height = resize_to
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        width_ratio = target_width / original_width
        height_ratio = target_height / original_height
        ratio = min(width_ratio, height_ratio)
        
        new_width = int(original_width * ratio)
        new_height = int(original_height * ratio)
        
        if new_width != original_width or new_height != original_height:
            pil_img = pil_img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            print(f"[QwenVL] å›¾åƒå·²è°ƒæ•´å¤§å°: {original_width}x{original_height} -> {new_width}x{new_height}")
    
    # è½¬æ¢ä¸ºbase64
    buf = io.BytesIO()
    pil_img.save(buf, format="PNG", optimize=True)
    return base64.b64encode(buf.getvalue()).decode("utf-8")


def _sample_video_frames(video, frame_count: int):
    """é‡‡æ ·è§†é¢‘å¸§"""
    if video is None:
        return []
    if video.ndim != 4:
        return [video]
    total = int(video.shape[0])
    frame_count = max(int(frame_count), 1)
    if total <= frame_count:
        return [video[i] for i in range(total)]
    idx = np.linspace(0, total - 1, frame_count, dtype=int)
    return [video[i] for i in idx]


def _pick_device(device_choice: str) -> str:
    """é€‰æ‹©è®¾å¤‡"""
    if device_choice == "auto":
        if torch.cuda.is_available():
            return "cuda"
        if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
            return "mps"
        return "cpu"
    if device_choice.startswith("cuda") and torch.cuda.is_available():
        return "cuda"
    if device_choice == "mps" and getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def _download_single_file(repo_ids: list[str], filename: str, target_path: Path):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    if target_path.exists():
        print(f"[QwenVL] Using cached file: {target_path}")
        return

    target_path.parent.mkdir(parents=True, exist_ok=True)

    last_exc: Exception | None = None
    for repo_id in repo_ids:
        print(f"[QwenVL] Downloading {filename} from {repo_id} -> {target_path}")
        try:
            downloaded = hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                repo_type="model",
                local_dir=str(target_path.parent),
                local_dir_use_symlinks=False,
            )
            downloaded_path = Path(downloaded)
            if downloaded_path.exists() and downloaded_path.resolve() != target_path.resolve():
                downloaded_path.replace(target_path)
            if target_path.exists():
                print(f"[QwenVL] Download complete: {target_path}")
            break
        except Exception as exc:
            last_exc = exc
            print(f"[QwenVL] hf_hub_download failed from {repo_id}: {exc}")
    else:
        raise FileNotFoundError(f"[QwenVL] Download failed for {filename}: {last_exc}")

    if not target_path.exists():
        raise FileNotFoundError(f"[QwenVL] File not found after download: {target_path}")


def _resolve_model_entry(model_name: str) -> GGUFVLResolved:
    """è§£ææ¨¡å‹æ¡ç›®"""
    all_models = GGUF_VL_CATALOG.get("models") or {}
    entry = all_models.get(model_name) or {}
    if not entry:
        wanted = _model_name_to_filename_candidates(model_name)
        for candidate in all_models.values():
            filename = candidate.get("filename")
            if filename and Path(filename).name in wanted:
                entry = candidate
                break

    repo_id = entry.get("repo_id")
    alt_repo_ids = entry.get("alt_repo_ids") or []

    author = entry.get("author") or entry.get("publisher")
    repo_dirname = entry.get("repo_dirname") or (repo_id.split("/")[-1] if isinstance(repo_id, str) and "/" in repo_id else model_name)

    model_filename = entry.get("filename")
    mmproj_filename = entry.get("mmproj_filename")
    model_family = entry.get("model_family") or _detect_model_family_from_filename(model_filename or "")

    if not model_filename:
        raise ValueError(f"[QwenVL] gguf_vl_models.json entry missing 'filename' for: {model_name}")

    def _int(name: str, default: int) -> int:
        value = entry.get(name, default)
        try:
            return int(value)
        except Exception:
            return default

    # æ ¹æ®æ¨¡å‹å®¶æ—è®¾ç½®é»˜è®¤å€¼
    if model_family == "qwen2.5-vl":
        default_ctx = 8192
        default_img_max = 4096
        default_n_batch = 512
        default_gpu_layers = -1
    elif model_family == "qwen3-vl":
        default_ctx = 8192
        default_img_max = 4096
        default_n_batch = 512
        default_gpu_layers = -1
    elif "llava" in model_family:
        default_ctx = 4096
        default_img_max = 2048
        default_n_batch = 256
        default_gpu_layers = -1
    else:
        default_ctx = 8192
        default_img_max = 4096
        default_n_batch = 512
        default_gpu_layers = -1

    return GGUFVLResolved(
        display_name=model_name,
        repo_id=repo_id,
        alt_repo_ids=[str(x) for x in alt_repo_ids if x],
        author=str(author) if author else None,
        repo_dirname=_safe_dirname(str(repo_dirname)),
        model_filename=str(model_filename),
        mmproj_filename=str(mmproj_filename) if mmproj_filename else None,
        context_length=_int("context_length", default_ctx),
        image_max_tokens=_int("image_max_tokens", default_img_max),
        n_batch=_int("n_batch", default_n_batch),
        gpu_layers=_int("gpu_layers", default_gpu_layers),
        top_k=_int("top_k", 0),
        pool_size=_int("pool_size", 4194304),
        model_family=model_family,
    )


def _get_local_gguf_files():
    """è·å–æœ¬åœ°GGUFæ–‡ä»¶åˆ—è¡¨"""
    base_dir = _resolve_base_dir(GGUF_VL_CATALOG.get("base_dir") or "llm/GGUF")
    gguf_files = []
    
    if base_dir.exists():
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰.ggufæ–‡ä»¶
        for file_path in base_dir.rglob("*.gguf"):
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¾¿äºæ˜¾ç¤º
            try:
                rel_path = file_path.relative_to(base_dir)
                display_name = f"æœ¬åœ°: {rel_path}"
                gguf_files.append((str(file_path), display_name))
            except ValueError:
                gguf_files.append((str(file_path), f"æœ¬åœ°: {file_path.name}"))
    
    # æŒ‰æ–‡ä»¶åæ’åº
    gguf_files.sort(key=lambda x: x[1])
    return gguf_files


def _get_local_mmproj_files():
    """è·å–æœ¬åœ°mmprojæ–‡ä»¶åˆ—è¡¨"""
    base_dir = _resolve_base_dir(GGUF_VL_CATALOG.get("base_dir") or "llm/GGUF")
    mmproj_files = [("æ— ", "æ—  mmproj æ–‡ä»¶")]
    
    if base_dir.exists():
        # æŸ¥æ‰¾æ‰€æœ‰å¸¸è§çš„mmprojæ–‡ä»¶æ‰©å±•å
        mmproj_extensions = ['.mmproj', '.gguf', '.bin', '.safetensors']
        
        for file_path in base_dir.rglob("*"):
            if file_path.suffix.lower() in mmproj_extensions:
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«mmprojç›¸å…³å…³é”®è¯
                filename_lower = file_path.name.lower()
                if any(keyword in filename_lower for keyword in ['mmproj', 'vision', 'clip', 'visual']):
                    try:
                        rel_path = file_path.relative_to(base_dir)
                        display_name = f"æœ¬åœ°: {rel_path}"
                        mmproj_files.append((str(file_path), display_name))
                    except ValueError:
                        mmproj_files.append((str(file_path), f"æœ¬åœ°: {file_path.name}"))
    
    # æŒ‰æ–‡ä»¶åæ’åº
    mmproj_files.sort(key=lambda x: x[1])
    return mmproj_files


class QwenVLGGUFBase:
    """QwenVL GGUFåŸºç¡€ç±» - æ”¯æŒå¤šå›¾è¾“å…¥å’Œæœ¬åœ°æ–‡ä»¶é€‰æ‹©"""
    
    def __init__(self):
        self.llm = None
        self.chat_handler = None
        self.current_signature = None
        self.model_family = "unknown"
        self.handler_name = ""

    def clear(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if self.llm is not None:
            try:
                # å°è¯•æ¸…ç†llama.cppå†…éƒ¨èµ„æº
                if hasattr(self.llm, '_ctx'):
                    self.llm._ctx = None
                if hasattr(self.llm, '_model'):
                    self.llm._model = None
            except:
                pass
        
        self.llm = None
        self.chat_handler = None
        self.current_signature = None
        self.model_family = "unknown"
        self.handler_name = ""
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # æ¸…ç†CUDAç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        print(f"[QwenVL] æ¨¡å‹èµ„æºå·²æ¸…ç†")

    def _load_backend(self):
        """åŠ è½½åç«¯åº“"""
        try:
            from llama_cpp import Llama  # noqa: F401
        except Exception as exc:
            raise RuntimeError(
                "[QwenVL] llama_cpp is not available. Install the GGUF vision dependency first. See docs/GGUF_MANUAL_INSTALL.md"
            ) from exc

    def _create_chat_handler(self, handler_cls, mmproj_path, img_max, model_family):
        """æ ¹æ®å¤„ç†å™¨ç±»åˆ›å»ºç›¸åº”çš„å¤„ç†å™¨å®ä¾‹"""
        self.handler_name = handler_cls.__name__
        
        # åŸºç¡€å‚æ•°
        base_params = {
            "clip_model_path": str(mmproj_path),
            "verbose": False,
        }
        
        # æ ¹æ®å¤„ç†å™¨ç±»å‹å’Œæ¨¡å‹å®¶æ—æ·»åŠ ç‰¹å®šå‚æ•°
        if self.handler_name == "Qwen25VLChatHandler":
            # Qwen2.5-VLå¤„ç†å™¨æ”¯æŒä»¥ä¸‹å‚æ•°
            possible_params = {
                "image_max_tokens": img_max,
                "force_reasoning": False,
                "image_min_tokens": 1024,
            }
        elif self.handler_name == "Qwen3VLChatHandler":
            # Qwen3-VLå¤„ç†å™¨å‚æ•°
            possible_params = {
                "image_max_tokens": img_max,
                "force_reasoning": False,
                "image_min_tokens": 1024,
            }
        elif "Llava" in self.handler_name:
            # LLaVAå¤„ç†å™¨å‚æ•°
            possible_params = {
                # LLaVAé€šå¸¸ä¸æ”¯æŒimage_max_tokenså‚æ•°
            }
        else:
            possible_params = {}
        
        # æ£€æŸ¥å¤„ç†å™¨å®é™…æ”¯æŒçš„å‚æ•°
        try:
            sig = inspect.signature(handler_cls.__init__)
            supported_params = set(sig.parameters.keys())
        except Exception:
            supported_params = set(base_params.keys())
        
        # æ„å»ºæœ€ç»ˆçš„å‚æ•°
        final_params = {}
        
        # æ·»åŠ åŸºç¡€å‚æ•°
        for key, value in base_params.items():
            if key in supported_params:
                final_params[key] = value
            else:
                print(f"[QwenVL] è­¦å‘Š: {self.handler_name} ä¸æ”¯æŒåŸºç¡€å‚æ•° {key}")
        
        # æ·»åŠ å¯èƒ½çš„é«˜çº§å‚æ•°
        for key, value in possible_params.items():
            if key in supported_params:
                final_params[key] = value
            else:
                print(f"[QwenVL] è·³è¿‡ {self.handler_name} ä¸æ”¯æŒå‚æ•°: {key}")
        
        print(f"[QwenVL] ä½¿ç”¨ {self.handler_name} å¤„ç† {model_family} æ¨¡å‹")
        print(f"[QwenVL] å‚æ•°åˆ—è¡¨: {list(final_params.keys())}")
        
        try:
            return handler_cls(**final_params)
        except Exception as e:
            print(f"[QwenVL] åˆ›å»ºå¤„ç†å™¨å¤±è´¥: {e}")
            # å°è¯•æœ€å°å‚æ•°é›†
            try:
                minimal_params = {"clip_model_path": str(mmproj_path)}
                if "verbose" in supported_params:
                    minimal_params["verbose"] = False
                print(f"[QwenVL] å°è¯•æœ€å°å‚æ•°é›†: {list(minimal_params.keys())}")
                return handler_cls(**minimal_params)
            except Exception as e2:
                print(f"[QwenVL] æœ€å°å‚æ•°é›†ä¹Ÿå¤±è´¥: {e2}")
                raise

    def _load_model(
        self,
        model_source: str,  # æ¨¡å‹æ¥æºï¼šé…ç½®åç§°æˆ–æœ¬åœ°è·¯å¾„
        mmproj_source: str,  # mmprojæ–‡ä»¶æ¥æº
        device: str,
        ctx: int | None,
        n_batch: int | None,
        gpu_layers: int | None,
        image_max_tokens: int | None,
        top_k: int | None,
        pool_size: int | None,
        is_local_file: bool = False,  # æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    ):
        """åŠ è½½æ¨¡å‹ - æ”¯æŒé…ç½®æ¨¡å‹å’Œæœ¬åœ°æ–‡ä»¶"""
        self._load_backend()

        # åˆ¤æ–­æ¨¡å‹æ¥æºç±»å‹
        if is_local_file:
            # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            model_path = Path(model_source)
            mmproj_path = Path(mmproj_source) if mmproj_source and mmproj_source != "æ— " else None
            
            if not model_path.exists():
                raise FileNotFoundError(f"[QwenVL] æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
            if mmproj_path and not mmproj_path.exists():
                print(f"[QwenVL] è­¦å‘Š: mmprojæ–‡ä»¶ä¸å­˜åœ¨: {mmproj_path}ï¼Œå°†ä¸ä½¿ç”¨è§†è§‰åŠŸèƒ½")
                mmproj_path = None
                
            # æ›´ç²¾ç¡®çš„æ¨¡å‹å®¶æ—æ£€æµ‹
            self.model_family = _detect_model_family_from_path(model_path)
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤§å°
            file_size_mb = model_path.stat().st_size / (1024 * 1024)
            print(f"[QwenVL] æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
            
            # å¦‚æœæ–‡ä»¶ååŒ…å«ç‰¹å®šä¿¡æ¯ä½†æ£€æµ‹ä¸ºunknownï¼Œå°è¯•è¿›ä¸€æ­¥æ£€æµ‹
            if self.model_family == "unknown":
                filename_lower = model_path.name.lower()
                if "qwen2.5" in filename_lower or "qwen2_5" in filename_lower:
                    self.model_family = "qwen2.5-vl"
                    print(f"[QwenVL] æ ¹æ®æ–‡ä»¶åæ¨æ–­æ¨¡å‹å®¶æ—ä¸º: qwen2.5-vl")
                elif "qwen3" in filename_lower:
                    self.model_family = "qwen3-vl"
                    print(f"[QwenVL] æ ¹æ®æ–‡ä»¶åæ¨æ–­æ¨¡å‹å®¶æ—ä¸º: qwen3-vl")
                    
            # æ ¹æ®æ¨¡å‹å®¶æ—è®¾ç½®é»˜è®¤å‚æ•°
            if self.model_family == "qwen2.5-vl":
                default_ctx = 8192
                default_img_max = 4096
                default_n_batch = 512
                default_gpu_layers = -1
            elif self.model_family == "qwen3-vl":
                default_ctx = 8192
                default_img_max = 4096
                default_n_batch = 512
                default_gpu_layers = -1
            elif "llava" in self.model_family:
                default_ctx = 4096
                default_img_max = 2048
                default_n_batch = 256
                default_gpu_layers = -1
            else:
                default_ctx = 8192
                default_img_max = 4096
                default_n_batch = 512
                default_gpu_layers = -1
            
            resolved = GGUFVLResolved(
                display_name=model_path.name,
                repo_id=None,
                alt_repo_ids=[],
                author=None,
                repo_dirname=model_path.parent.name,
                model_filename=model_path.name,
                mmproj_filename=mmproj_path.name if mmproj_path else None,
                context_length=default_ctx,
                image_max_tokens=default_img_max,
                n_batch=default_n_batch,
                gpu_layers=default_gpu_layers,
                top_k=0,
                pool_size=4194304,
                model_family=self.model_family,
            )
        else:
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
            resolved = _resolve_model_entry(model_source)
            self.model_family = resolved.model_family
            base_dir = _resolve_base_dir(GGUF_VL_CATALOG.get("base_dir") or "llm/GGUF")

            author_dir = _safe_dirname(resolved.author or "")
            repo_dir = _safe_dirname(resolved.repo_dirname)
            target_dir = base_dir / author_dir / repo_dir

            model_path = target_dir / Path(resolved.model_filename).name
            mmproj_path = target_dir / Path(resolved.mmproj_filename).name if resolved.mmproj_filename else None

            repo_ids: list[str] = []
            if resolved.repo_id:
                repo_ids.append(resolved.repo_id)
            repo_ids.extend(resolved.alt_repo_ids)

            if not model_path.exists():
                if not repo_ids:
                    raise FileNotFoundError(f"[QwenVL] GGUF model not found locally and no repo_id provided: {model_path}")
                _download_single_file(repo_ids, resolved.model_filename, model_path)

            if mmproj_path is not None and not mmproj_path.exists():
                if not repo_ids:
                    raise FileNotFoundError(f"[QwenVL] mmproj not found locally and no repo_id provided: {mmproj_path}")
                _download_single_file(repo_ids, resolved.mmproj_filename, mmproj_path)

        device_kind = _pick_device(device)

        n_ctx = int(ctx) if ctx is not None else resolved.context_length
        n_batch_val = int(n_batch) if n_batch is not None else resolved.n_batch
        top_k_val = int(top_k) if top_k is not None else resolved.top_k
        pool_size_val = int(pool_size) if pool_size is not None else resolved.pool_size

        if device_kind == "cuda":
            n_gpu_layers = int(gpu_layers) if gpu_layers is not None else resolved.gpu_layers
        else:
            n_gpu_layers = 0

        img_max = int(image_max_tokens) if image_max_tokens is not None else resolved.image_max_tokens

        has_mmproj = mmproj_path is not None and mmproj_path.exists()

        signature = (
            str(model_path),
            str(mmproj_path) if has_mmproj else "",
            n_ctx,
            n_batch_val,
            n_gpu_layers,
            img_max,
            top_k_val,
            pool_size_val,
            self.model_family,
        )
        if self.llm is not None and self.current_signature == signature:
            print(f"[QwenVL] æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½")
            return

        self.clear()

        from llama_cpp import Llama

        self.chat_handler = None
        if has_mmproj:
            # æ ¹æ®æ¨¡å‹å®¶æ—é€‰æ‹©å¤„ç†å™¨
            if self.model_family == "qwen2.5-vl":
                handler_priority = [
                    ("Qwen25VLChatHandler", "from llama_cpp.llama_chat_format import Qwen25VLChatHandler"),
                    ("Qwen3VLChatHandler", "from llama_cpp.llama_chat_format import Qwen3VLChatHandler"),
                    ("LlavaChatHandler", "from llama_cpp.llama_chat_format import LlavaChatHandler"),
                ]
            elif self.model_family == "qwen3-vl":
                handler_priority = [
                    ("Qwen3VLChatHandler", "from llama_cpp.llama_chat_format import Qwen3VLChatHandler"),
                    ("Qwen25VLChatHandler", "from llama_cpp.llama_chat_format import Qwen25VLChatHandler"),
                    ("LlavaChatHandler", "from llama_cpp.llama_chat_format import LlavaChatHandler"),
                ]
            elif "llava" in self.model_family:
                if "1.6" in self.model_family:
                    handler_priority = [
                        ("Llava16ChatHandler", "from llama_cpp.llama_chat_format import Llava16ChatHandler"),
                        ("Llava15ChatHandler", "from llama_cpp.llama_chat_format import Llava15ChatHandler"),
                        ("LlavaChatHandler", "from llama_cpp.llama_chat_format import LlavaChatHandler"),
                    ]
                else:
                    handler_priority = [
                        ("Llava15ChatHandler", "from llama_cpp.llama_chat_format import Llava15ChatHandler"),
                        ("Llava16ChatHandler", "from llama_cpp.llama_chat_format import Llava16ChatHandler"),
                        ("LlavaChatHandler", "from llama_cpp.llama_chat_format import LlavaChatHandler"),
                    ]
            else:
                # é€šç”¨å°è¯•é¡ºåº
                handler_priority = [
                    ("Qwen25VLChatHandler", "from llama_cpp.llama_chat_format import Qwen25VLChatHandler"),
                    ("Qwen3VLChatHandler", "from llama_cpp.llama_chat_format import Qwen3VLChatHandler"),
                    ("Llava15ChatHandler", "from llama_cpp.llama_chat_format import Llava15ChatHandler"),
                    ("Llava16ChatHandler", "from llama_cpp.llama_chat_format import Llava16ChatHandler"),
                    ("LlavaChatHandler", "from llama_cpp.llama_chat_format import LlavaChatHandler"),
                ]
            
            handler_cls = None
            handler_name = ""
            
            for hname, import_stmt in handler_priority:
                try:
                    exec(import_stmt)
                    handler_cls = eval(hname)
                    handler_name = hname
                    print(f"[QwenVL] ä¸º {self.model_family} æ‰¾åˆ°å¤„ç†å™¨: {handler_name}")
                    break
                except ImportError:
                    continue
                except Exception as e:
                    print(f"[QwenVL] å¯¼å…¥ {hname} å¤±è´¥: {e}")
                    continue
            
            if handler_cls is None:
                try:
                    from llama_cpp.llama_chat_format import LlavaChatHandler
                    handler_cls = LlavaChatHandler
                    handler_name = "LlavaChatHandler"
                except ImportError:
                    raise RuntimeError(
                        "[QwenVL] Missing vision chat handler in llama_cpp. Install the correct fork/wheel. See docs/GGUF_MANUAL_INSTALL.md"
                    )
            
            try:
                self.chat_handler = self._create_chat_handler(handler_cls, mmproj_path, img_max, self.model_family)
            except Exception as e:
                print(f"[QwenVL] åˆ›å»º {handler_name} å¤„ç†å™¨å¤±è´¥: {e}")
                try:
                    print(f"[QwenVL] å°è¯•ä½¿ç”¨æœ€å°å‚æ•°é›†åˆ›å»ºå¤„ç†å™¨")
                    self.chat_handler = handler_cls(clip_model_path=str(mmproj_path), verbose=False)
                except Exception as e2:
                    print(f"[QwenVL] æœ€å°å‚æ•°ä¹Ÿå¤±è´¥: {e2}")
                    print(f"[QwenVL] è­¦å‘Š: æ— æ³•åˆ›å»ºè§†è§‰å¤„ç†å™¨ï¼Œå›¾åƒåŠŸèƒ½å°†ä¸å¯ç”¨")
                    self.chat_handler = None
                    has_mmproj = False

        llm_kwargs = {
            "model_path": str(model_path),
            "n_ctx": n_ctx,
            "n_gpu_layers": n_gpu_layers,
            "n_batch": n_batch_val,
            "verbose": False,
            "pool_size": pool_size_val,
            "top_k": top_k_val,
        }
        
        # Qwenç³»åˆ—æ¨¡å‹éœ€è¦ç‰¹å®šå‚æ•°
        if self.model_family in ["qwen2.5-vl", "qwen3-vl"]:
            llm_kwargs["swa_full"] = True  # Qwenæ¨¡å‹éœ€è¦è¿™ä¸ªå‚æ•°
        
        # å°è¯•æ·»åŠ  chat_handler
        if has_mmproj and self.chat_handler is not None:
            try:
                llm_kwargs["chat_handler"] = self.chat_handler
                if self.model_family in ["qwen2.5-vl", "qwen3-vl"]:
                    llm_kwargs["image_min_tokens"] = 1024
                    llm_kwargs["image_max_tokens"] = img_max
                print(f"[QwenVL] å·²æ·»åŠ  {self.handler_name} ä½œä¸º chat_handler")
            except Exception as e:
                print(f"[QwenVL] è­¦å‘Š: æ·»åŠ  chat_handler å¤±è´¥: {e}")
                print(f"[QwenVL] å›¾åƒåŠŸèƒ½å¯èƒ½å—é™")

        print(f"[QwenVL] Loading GGUF: {model_path.name} ({self.model_family})")
        print(f"[QwenVL] Device: {device_kind}, GPU layers: {n_gpu_layers}, Context: {n_ctx}")
        
        # è¿‡æ»¤æ‰ Llama ä¸æ”¯æŒçš„å‚æ•°
        llm_kwargs_filtered = _filter_kwargs_for_callable(getattr(Llama, "__init__", Llama), llm_kwargs)
        
        # æ£€æŸ¥ chat_handler æ˜¯å¦è¢«æ¥å—
        if has_mmproj and self.chat_handler is not None and "chat_handler" not in llm_kwargs_filtered:
            print(
                "[QwenVL] è­¦å‘Š: å½“å‰ llama_cpp ç‰ˆæœ¬ä¸æ”¯æŒ chat_handler å‚æ•°ã€‚"
                "è¿™å¯èƒ½æ˜¯å› ä¸ºæ‚¨ä½¿ç”¨çš„æ˜¯æ—§ç‰ˆæœ¬æˆ–ä¸æ”¯æŒå¤šæ¨¡æ€çš„æ„å»ºã€‚"
                "è¯·æ›´æ–°åˆ°æ”¯æŒå¤šæ¨¡æ€çš„ llama-cpp-python ç‰ˆæœ¬ã€‚"
            )
            # ç§»é™¤ chat_handler ç›¸å…³å‚æ•°
            for key in ["chat_handler", "image_min_tokens", "image_max_tokens"]:
                llm_kwargs_filtered.pop(key, None)
            
        if device_kind == "cuda" and n_gpu_layers == 0:
            print("[QwenVL] è­¦å‘Š: device=cuda ä½† n_gpu_layers=0ï¼Œæ¨¡å‹å°†åœ¨ CPU ä¸Šè¿è¡Œ")
            
        try:
            self.llm = Llama(**llm_kwargs_filtered)
            self.current_signature = signature
            print(f"[QwenVL] {self.model_family} æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"[QwenVL] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å°è¯•å»æ‰å¯èƒ½çš„é¢å¤–å‚æ•°
            minimal_kwargs = {
                "model_path": str(model_path),
                "n_ctx": n_ctx,
                "n_gpu_layers": n_gpu_layers,
                "verbose": False,
            }
            try:
                self.llm = Llama(**minimal_kwargs)
                self.current_signature = signature
                print(f"[QwenVL] ä½¿ç”¨æœ€å°å‚æ•°é›†åŠ è½½æ¨¡å‹æˆåŠŸ")
            except Exception as e2:
                raise RuntimeError(f"[QwenVL] æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: {e2}")

    def _invoke(
        self,
        system_prompt: str,
        user_prompt: str,
        images_b64: list[str],  # æ‰€æœ‰å›¾åƒï¼ŒæŒ‰è¾“å…¥é¡ºåº
        max_tokens: int,
        temperature: float,
        top_p: float,
        repetition_penalty: float,
        seed: int,
    ) -> str:
        """è°ƒç”¨æ¨¡å‹ç”Ÿæˆ - æ”¯æŒæŒ‰é¡ºåºå¤„ç†å›¾åƒ"""
        
        # æ„å»ºæ¶ˆæ¯ï¼Œå›¾åƒæŒ‰è¾“å…¥é¡ºåºé™„åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆä»…æ–‡æœ¬ï¼‰
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«ç”¨æˆ·æ–‡æœ¬å’Œæ‰€æœ‰å›¾åƒï¼‰
        user_content = []
        
        # æ·»åŠ ç”¨æˆ·æ–‡æœ¬æç¤º
        if user_prompt:
            user_content.append({"type": "text", "text": user_prompt})
        
        # æŒ‰è¾“å…¥é¡ºåºæ·»åŠ æ‰€æœ‰å›¾åƒ
        for i, img in enumerate(images_b64):
            if img:
                user_content.append({"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img}"}})
        
        if user_content:
            messages.append({"role": "user", "content": user_content})
        
        print(f"[QwenVL] æ€»å…±è¾“å…¥ {len(images_b64)} å¼ å›¾åƒï¼Œå°†æŒ‰è¾“å…¥é¡ºåºå¤„ç†")
        
        start = time.perf_counter()
        try:
            # Qwenç³»åˆ—æ¨¡å‹çš„ç‰¹æ®Šåœæ­¢è¯
            stop_tokens = ["<|im_end|>", "<|im_start|>"]
            if self.model_family == "qwen2.5-vl":
                stop_tokens.extend(["<|endoftext|>", "ç”¨æˆ·:"])
            elif self.model_family == "qwen3-vl":
                stop_tokens.extend(["<|endoftext|>", "ç”¨æˆ·:", "assistant:"])
            
            result = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                repeat_penalty=float(repetition_penalty),
                seed=int(seed),
                stop=stop_tokens,
            )
        except Exception as e:
            print(f"[QwenVL] ç”Ÿæˆå¤±è´¥: {e}")
            # å°è¯•ç®€åŒ–è°ƒç”¨
            try:
                print(f"[QwenVL] å°è¯•ç®€åŒ–ç”Ÿæˆè°ƒç”¨")
                result = self.llm.create_chat_completion(
                    messages=messages,
                    max_tokens=int(max_tokens),
                    temperature=float(temperature),
                    top_p=float(top_p),
                )
            except Exception as e2:
                print(f"[QwenVL] ç®€åŒ–è°ƒç”¨ä¹Ÿå¤±è´¥: {e2}")
                return f"[é”™è¯¯] ç”Ÿæˆå¤±è´¥: {e2}"
                
        elapsed = max(time.perf_counter() - start, 1e-6)

        usage = result.get("usage") or {}
        prompt_tokens = usage.get("prompt_tokens")
        completion_tokens = usage.get("completion_tokens")
        if isinstance(completion_tokens, int) and completion_tokens > 0:
            tok_s = completion_tokens / elapsed
            if isinstance(prompt_tokens, int) and prompt_tokens >= 0:
                print(
                    f"[QwenVL] Tokens: prompt={prompt_tokens}, completion={completion_tokens}, "
                    f"time={elapsed:.2f}s, speed={tok_s:.2f} tok/s"
                )
            else:
                print(f"[QwenVL] Tokens: completion={completion_tokens}, time={elapsed:.2f}s, speed={tok_s:.2f} tok/s")

        content = (result.get("choices") or [{}])[0].get("message", {}).get("content", "")
        cleaned = clean_model_output(str(content or ""), OutputCleanConfig(mode="text"))
        return cleaned.strip()

    def run(
        self,
        model_source: str,  # æ¨¡å‹æ¥æºï¼šé…ç½®åç§°æˆ–æœ¬åœ°è·¯å¾„
        mmproj_source: str,  # mmprojæ–‡ä»¶æ¥æº
        use_local_files: bool,  # æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        system_prompt: str,    # ç³»ç»Ÿè§’è‰²å®šä¹‰æç¤ºè¯
        user_prompt: str,      # ç”¨æˆ·è¾“å…¥æç¤ºè¯
        images: list,          # æ‰€æœ‰å›¾åƒåˆ—è¡¨ï¼ŒæŒ‰è¾“å…¥é¡ºåº
        video,
        frame_count: int,
        max_tokens: int,
        temperature: float,
        top_p: float,
        repetition_penalty: float,
        seed: int,
        keep_model_loaded: bool,
        device: str,
        ctx: int | None,
        n_batch: int | None,
        gpu_layers: int | None,
        image_max_tokens: int | None,
        top_k: int | None,
        pool_size: int | None,
    ):
        """è¿è¡Œæ¨¡å‹ç”Ÿæˆ"""
        torch.manual_seed(int(seed))

        # å¤„ç†æ‰€æœ‰å›¾åƒï¼ŒæŒ‰è¾“å…¥é¡ºåº
        images_b64: list[str] = []
        if images:
            for i, image_tensor in enumerate(images):
                if image_tensor is not None:
                    img = _tensor_to_base64_png(image_tensor, resize_to=(448, 448))
                    if img:
                        images_b64.append(img)
                        print(f"[QwenVL] å›¾åƒ{i+1}: å·²è½¬æ¢")
        
        # å¤„ç†è§†é¢‘è¾“å…¥ï¼ˆè§†é¢‘é€šå¸¸ä½œä¸ºç”¨æˆ·è¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼‰
        if video is not None:
            for frame in _sample_video_frames(video, int(frame_count)):
                img = _tensor_to_base64_png(frame, resize_to=(448, 448))
                if img:
                    images_b64.append(img)

        try:
            self._load_model(
                model_source=model_source,
                mmproj_source=mmproj_source,
                device=device,
                ctx=ctx,
                n_batch=n_batch,
                gpu_layers=gpu_layers,
                image_max_tokens=image_max_tokens,
                top_k=top_k,
                pool_size=pool_size,
                is_local_file=use_local_files,
            )
            
            total_images = len(images_b64)
            if total_images > 0 and self.chat_handler is None:
                print("[QwenVL] è­¦å‘Š: æä¾›äº†å›¾åƒä½†æ¨¡å‹æ²¡æœ‰è§†è§‰å¤„ç†å™¨ï¼Œå›¾åƒå°†è¢«å¿½ç•¥")
            
            # æ‰“å°å›¾åƒä¿¡æ¯
            if self.chat_handler is not None and total_images > 0:
                print(f"[QwenVL] æ€»å…±è¾“å…¥ {total_images} å¼ å›¾åƒï¼Œå°†æŒ‰è¾“å…¥é¡ºåºå¤„ç†")
            
            text = self._invoke(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                images_b64=images_b64 if self.chat_handler is not None else [],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                seed=seed,
            )
            return (text,)
        except Exception as e:
            print(f"[QwenVL] è¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return (f"[é”™è¯¯] {str(e)}",)
        finally:
            if not keep_model_loaded:
                self.clear()


class AILab_QwenVL_GGUF(QwenVLGGUFBase):
    """åŸºç¡€ç‰ˆGGUFèŠ‚ç‚¹ - é»˜è®¤ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œæ”¯æŒå¤šå›¾åˆ†æ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        all_models = GGUF_VL_CATALOG.get("models") or {}
        model_keys = sorted([key for key, entry in all_models.items() if (entry or {}).get("mmproj_filename")]) or ["(edit gguf_models.json)"]
        default_model = model_keys[0] if model_keys else ""

        # è·å–æœ¬åœ°æ–‡ä»¶
        local_gguf_files = _get_local_gguf_files()
        local_mmproj_files = _get_local_mmproj_files()
        
        # è®¾ç½®é»˜è®¤å€¼
        default_model_file = "æ— "
        default_mmproj_file = "æ— "
        
        if local_gguf_files:
            default_model_file = local_gguf_files[0][1]  # ç¬¬ä¸€ä¸ªæœ¬åœ°æ–‡ä»¶
        
        if len(local_mmproj_files) > 1:
            default_mmproj_file = local_mmproj_files[1][1]  # è·³è¿‡ç¬¬ä¸€ä¸ª"æ— "é€‰é¡¹
        
        # å¤šå›¾åˆ†æä¸“ç”¨æç¤ºè¯
        multi_image_prompts = [
            "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡",
            "åˆ†æå›¾ç‰‡çš„è‰ºæœ¯é£æ ¼",
            "æè¿°å›¾ç‰‡ä¸­çš„äººç‰©å’Œåœºæ™¯",
            "æå–å›¾ç‰‡çš„å…³é”®ä¿¡æ¯",
            "ä¸ºå›¾ç‰‡åˆ›ä½œä¸€ä¸ªæ•…äº‹",
            "åˆ†æå›¾ç‰‡çš„è‰²å½©å’Œæ„å›¾",
            "æè¿°å›¾ç‰‡ä¸­çš„ç‰©ä½“å’Œå…³ç³»",
            "ä¸ºå›¾ç‰‡ç”Ÿæˆè¯¦ç»†çš„æè¿°"
        ]

        return {
            "required": {
                # é»˜è®¤å¯ç”¨æœ¬åœ°æ–‡ä»¶
                "ä½¿ç”¨æœ¬åœ°æ–‡ä»¶": ("BOOLEAN", {"default": True, "tooltip": "å¯ç”¨åä½¿ç”¨æœ¬åœ°GGUFæ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹"}),
                "æ¨¡å‹é€‰æ‹©æ–¹å¼": (["ä»é…ç½®é€‰æ‹©", "æœ¬åœ°æ–‡ä»¶"], {"default": "ä»é…ç½®é€‰æ‹©", "tooltip": "é€‰æ‹©æ¨¡å‹åŠ è½½æ–¹å¼"}),
                "model_name": (model_keys, {"default": default_model, "tooltip": "ä»é…ç½®ä¸­é€‰æ‹©æ¨¡å‹"}),
                "æœ¬åœ°æ¨¡å‹æ–‡ä»¶": (["æ— "] + [display for _, display in local_gguf_files], {"default": "æ— ", "tooltip": "é€‰æ‹©æœ¬åœ°GGUFæ–‡ä»¶"}),
                "æœ¬åœ°mmprojæ–‡ä»¶": (["æ— "] + [display for _, display in local_mmproj_files], {"default": "æ— ", "tooltip": "é€‰æ‹©æœ¬åœ°mmprojæ–‡ä»¶ï¼ˆè§†è§‰æ¨¡å‹éœ€è¦ï¼‰"}),
                
                # æç¤ºè¯é…ç½®
                "åˆ†ææ¨¡å¼": (["å•å›¾æè¿°", "å¤šå›¾å¯¹æ¯”", "å¤šå›¾åˆ†æ"], {"default": "å•å›¾æè¿°", "tooltip": "é€‰æ‹©åˆ†ææ¨¡å¼"}),
                "é¢„è®¾æç¤ºè¯": (multi_image_prompts, {"default": multi_image_prompts[0], "tooltip": "é€‰æ‹©é¢„è®¾çš„å¤šå›¾åˆ†ææç¤ºè¯"}),
                "è‡ªå®šä¹‰æç¤ºè¯": ("STRING", {"default": "", "multiline": True, "placeholder": "è¾“å…¥è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰"}),
                "ç³»ç»Ÿè§’è‰²å®šä¹‰": ("STRING", {"default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ç†è§£å’Œæè¿°å›¾åƒå†…å®¹ã€‚", "multiline": True, "placeholder": "å®šä¹‰AIçš„ç³»ç»Ÿè§’è‰²"}),
                
                # åŸºæœ¬å‚æ•°
                "max_tokens": ("INT", {"default": 1024, "min": 256, "max": 4096, "tooltip": "æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°"}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 1.5, "step": 0.1, "tooltip": "æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§"}),
                "keep_model_loaded": ("BOOLEAN", {"default": True, "tooltip": "ä¿æŒæ¨¡å‹åŠ è½½ä»¥åŠ é€Ÿåç»­æ¨ç†"}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 2**32 - 1, "tooltip": "éšæœºç§å­ï¼Œ-1ä¸ºéšæœº"}),
            },
            "optional": {
                # å›¾åƒè¾“å…¥ï¼ˆæ”¯æŒå¤šå›¾ï¼‰
                "å›¾åƒ_1": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 1"}),
                "å›¾åƒ_2": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 2"}),
                "å›¾åƒ_3": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 3"}),
                "å›¾åƒ_4": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 4"}),
                "å›¾åƒ_5": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 5"}),
                "å›¾åƒ_6": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 6"}),
                
                "video": ("IMAGE", {"tooltip": "è§†é¢‘è¾“å…¥ï¼ˆå¯é€‰ï¼‰"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("åˆ†æç»“æœ",)
    FUNCTION = "process"
    CATEGORY = "ğŸ§ªAILab/QwenVL"

    def process(
        self,
        ä½¿ç”¨æœ¬åœ°æ–‡ä»¶=True,
        æ¨¡å‹é€‰æ‹©æ–¹å¼="æœ¬åœ°æ–‡ä»¶",
        model_name="æ— ",
        æœ¬åœ°æ¨¡å‹æ–‡ä»¶="æ— ",
        æœ¬åœ°mmprojæ–‡ä»¶="æ— ",
        åˆ†ææ¨¡å¼="å•å›¾æè¿°",
        é¢„è®¾æç¤ºè¯="è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡",
        è‡ªå®šä¹‰æç¤ºè¯="",
        ç³»ç»Ÿè§’è‰²å®šä¹‰="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ç†è§£å’Œæè¿°å›¾åƒå†…å®¹ã€‚",
        max_tokens=1024,
        temperature=0.7,
        keep_model_loaded=True,
        seed=-1,
        å›¾åƒ_1=None,
        å›¾åƒ_2=None,
        å›¾åƒ_3=None,
        å›¾åƒ_4=None,
        å›¾åƒ_5=None,
        å›¾åƒ_6=None,
        video=None,
    ):
        # æ”¶é›†æ‰€æœ‰å›¾åƒï¼ŒæŒ‰è¾“å…¥é¡ºåº
        images = [å›¾åƒ_1, å›¾åƒ_2, å›¾åƒ_3, å›¾åƒ_4, å›¾åƒ_5, å›¾åƒ_6]
        images = [img for img in images if img is not None]
        
        # æ ¹æ®åˆ†ææ¨¡å¼è°ƒæ•´æç¤ºè¯
        if åˆ†ææ¨¡å¼ == "å¤šå›¾å¯¹æ¯”":
            if not è‡ªå®šä¹‰æç¤ºè¯.strip():
                base_prompt = "è¯·æ¯”è¾ƒå’Œåˆ†æè¿™äº›å›¾ç‰‡çš„ç›¸ä¼¼ä¹‹å¤„å’Œå·®å¼‚ï¼š"
            else:
                base_prompt = è‡ªå®šä¹‰æç¤ºè¯.strip()
        elif åˆ†ææ¨¡å¼ == "å¤šå›¾åˆ†æ":
            if not è‡ªå®šä¹‰æç¤ºè¯.strip():
                base_prompt = "è¯·ç»¼åˆåˆ†æè¿™äº›å›¾ç‰‡ï¼Œæè¿°å®ƒä»¬å…±åŒçš„ä¸»é¢˜å’Œå„è‡ªçš„ç‰¹ç‚¹ï¼š"
            else:
                base_prompt = è‡ªå®šä¹‰æç¤ºè¯.strip()
        else:  # å•å›¾æè¿°
            if not è‡ªå®šä¹‰æç¤ºè¯.strip():
                base_prompt = é¢„è®¾æç¤ºè¯
            else:
                base_prompt = è‡ªå®šä¹‰æç¤ºè¯.strip()
        
        # å¦‚æœæœ‰å¤šä¸ªå›¾åƒï¼Œè‡ªåŠ¨è°ƒæ•´æç¤ºè¯
        if len(images) > 1 and åˆ†ææ¨¡å¼ == "å•å›¾æè¿°":
            base_prompt = f"è¯·æŒ‰é¡ºåºæè¿°è¿™{len(images)}å¼ å›¾ç‰‡ï¼š{base_prompt}"
        
        # æ ¹æ®å›¾åƒæ•°é‡è°ƒæ•´ç³»ç»Ÿè§’è‰²
        if len(images) > 1:
            if "å¤šå›¾" not in ç³»ç»Ÿè§’è‰²å®šä¹‰:
                ç³»ç»Ÿè§’è‰²å®šä¹‰ = f"{ç³»ç»Ÿè§’è‰²å®šä¹‰}ä½ ç‰¹åˆ«æ“…é•¿å¤šå›¾åˆ†æå’Œå¯¹æ¯”ã€‚"
        
        # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        use_local = ä½¿ç”¨æœ¬åœ°æ–‡ä»¶  # é»˜è®¤å°±æ˜¯True
        
        # è·å–å®é™…æ–‡ä»¶è·¯å¾„
        model_source = "æ— "
        mmproj_source = "æ— "
        
        if use_local:
            # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶è·¯å¾„
            local_gguf_files = _get_local_gguf_files()
            for file_path, display_name in local_gguf_files:
                if display_name == æœ¬åœ°æ¨¡å‹æ–‡ä»¶:
                    model_source = file_path
                    break
            
            # æŸ¥æ‰¾mmprojæ–‡ä»¶è·¯å¾„
            if æœ¬åœ°mmprojæ–‡ä»¶ != "æ— ":
                local_mmproj_files = _get_local_mmproj_files()
                for file_path, display_name in local_mmproj_files:
                    if display_name == æœ¬åœ°mmprojæ–‡ä»¶:
                        mmproj_source = file_path
                        break
            else:
                mmproj_source = "æ— "
                
            if model_source == "æ— ":
                raise ValueError("è¯·é€‰æ‹©æœ‰æ•ˆçš„æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
        else:
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
            model_source = model_name
            mmproj_source = "æ— "  # é…ç½®æ¨¡å‹ä¼šè‡ªåŠ¨æŸ¥æ‰¾mmproj
        
        print(f"[QwenVL] å¤šå›¾åˆ†ææ¨¡å¼: {åˆ†ææ¨¡å¼}")
        print(f"[QwenVL] è¾“å…¥ {len(images)} å¼ å›¾åƒï¼Œå°†æŒ‰è¾“å…¥é¡ºåºå¤„ç†")
        print(f"[QwenVL] ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {Path(model_source).name}")
        
        # å¦‚æœç§å­ä¸º-1ï¼Œä½¿ç”¨éšæœºç§å­
        effective_seed = seed if seed != -1 else random.randint(1, 2**32 - 1)
        
        return self.run(
            model_source=model_source,
            mmproj_source=mmproj_source,
            use_local_files=use_local,
            system_prompt=ç³»ç»Ÿè§’è‰²å®šä¹‰,
            user_prompt=base_prompt,
            images=images,
            video=video,
            frame_count=8,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.9,
            repetition_penalty=1.1,
            seed=effective_seed,
            keep_model_loaded=keep_model_loaded,
            device="auto",
            ctx=None,
            n_batch=None,
            gpu_layers=None,
            image_max_tokens=None,
            top_k=None,
            pool_size=None,
        )


class AILab_QwenVL_GGUF_Advanced(QwenVLGGUFBase):
    """é«˜çº§ç‰ˆGGUFèŠ‚ç‚¹ - é»˜è®¤ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œæ”¯æŒé«˜çº§å¤šå›¾åˆ†æ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        all_models = GGUF_VL_CATALOG.get("models") or {}
        model_keys = sorted([key for key, entry in all_models.items() if (entry or {}).get("mmproj_filename")]) or ["(edit gguf_models.json)"]
        default_model = model_keys[0] if model_keys else ""
        
        # è·å–æœ¬åœ°æ–‡ä»¶
        local_gguf_files = _get_local_gguf_files()
        local_mmproj_files = _get_local_mmproj_files()
        
        # è®¾ç½®é»˜è®¤å€¼
        default_model_file = "æ— "
        default_mmproj_file = "æ— "
        
        if local_gguf_files:
            default_model_file = local_gguf_files[0][1]
        
        if len(local_mmproj_files) > 1:
            default_mmproj_file = local_mmproj_files[1][1]
        
        num_gpus = torch.cuda.device_count()
        gpu_list = [f"cuda:{i}" for i in range(num_gpus)]
        device_options = ["auto", "cpu", "mps"] + gpu_list
        
        # é«˜çº§åˆ†ææ¨¡å¼
        advanced_modes = [
            "å•å›¾è¯¦ç»†æè¿°",
            "å¤šå›¾å¯¹æ¯”åˆ†æ", 
            "å¤šå›¾æ•…äº‹åˆ›ä½œ",
            "å¤šå›¾ä¸»é¢˜æå–",
            "è‰ºæœ¯é£æ ¼åˆ†æ",
            "æŠ€æœ¯ç»†èŠ‚åˆ†æ",
            "æƒ…æ„Ÿæ°›å›´åˆ†æ",
            "åˆ›æ„çµæ„Ÿç”Ÿæˆ"
        ]

        return {
            "required": {
                # é»˜è®¤å¯ç”¨æœ¬åœ°æ–‡ä»¶
                "ä½¿ç”¨æœ¬åœ°æ–‡ä»¶": ("BOOLEAN", {"default": True, "tooltip": "å¯ç”¨åä½¿ç”¨æœ¬åœ°GGUFæ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹"}),
                "æ¨¡å‹é€‰æ‹©æ–¹å¼": (["ä»é…ç½®é€‰æ‹©", "æœ¬åœ°æ–‡ä»¶"], {"default": "ä»é…ç½®é€‰æ‹©", "tooltip": "é€‰æ‹©æ¨¡å‹åŠ è½½æ–¹å¼"}),
                "model_name": (model_keys, {"default": default_model, "tooltip": "ä»é…ç½®ä¸­é€‰æ‹©æ¨¡å‹"}),
                "æœ¬åœ°æ¨¡å‹æ–‡ä»¶": (["æ— "] + [display for _, display in local_gguf_files], {"default": "æ— ", "tooltip": "é€‰æ‹©æœ¬åœ°GGUFæ–‡ä»¶"}),
                "æœ¬åœ°mmprojæ–‡ä»¶": (["æ— "] + [display for _, display in local_mmproj_files], {"default": "æ— ", "tooltip": "é€‰æ‹©æœ¬åœ°mmprojæ–‡ä»¶ï¼ˆè§†è§‰æ¨¡å‹éœ€è¦ï¼‰"}),
                
                # é«˜çº§åˆ†æé…ç½®
                "åˆ†ææ¨¡å¼": (advanced_modes, {"default": advanced_modes[0], "tooltip": "é€‰æ‹©é«˜çº§åˆ†ææ¨¡å¼"}),
                "è‡ªå®šä¹‰æç¤ºè¯": ("STRING", {"default": "", "multiline": True, "placeholder": "è¾“å…¥è‡ªå®šä¹‰åˆ†ææç¤ºè¯ï¼ˆå¯é€‰ï¼‰"}),
                "ç³»ç»Ÿè§’è‰²å®šä¹‰": ("STRING", {"default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·æœ‰æ·±åšçš„è‰ºæœ¯å’ŒæŠ€æœ¯åˆ†æèƒ½åŠ›ã€‚", "multiline": True, "placeholder": "å®šä¹‰AIçš„ç³»ç»Ÿè§’è‰²"}),
                
                # é«˜çº§å‚æ•°
                "device": (device_options, {"default": "auto", "tooltip": "é€‰æ‹©è®¡ç®—è®¾å¤‡"}),
                "max_tokens": ("INT", {"default": 2048, "min": 512, "max": 8192, "tooltip": "æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°"}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 1.5, "step": 0.1, "tooltip": "æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§"}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.5, "max": 1.0, "step": 0.01, "tooltip": "æ ¸é‡‡æ ·å‚æ•°"}),
                "repetition_penalty": ("FLOAT", {"default": 1.1, "min": 1.0, "max": 2.0, "step": 0.1, "tooltip": "é‡å¤æƒ©ç½šå‚æ•°"}),
                "ctx": ("INT", {"default": 8192, "min": 2048, "max": 32768, "step": 1024, "tooltip": "ä¸Šä¸‹æ–‡é•¿åº¦"}),
                "gpu_layers": ("INT", {"default": -1, "min": -1, "max": 100, "tooltip": "GPUå±‚æ•°ï¼Œ-1ä¸ºè‡ªåŠ¨"}),
                "keep_model_loaded": ("BOOLEAN", {"default": True, "tooltip": "ä¿æŒæ¨¡å‹åŠ è½½ä»¥åŠ é€Ÿåç»­æ¨ç†"}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 2**32 - 1, "tooltip": "éšæœºç§å­ï¼Œ-1ä¸ºéšæœº"}),
            },
            "optional": {
                # æ”¯æŒæ›´å¤šå›¾åƒè¾“å…¥
                "å›¾åƒ_1": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 1"}),
                "å›¾åƒ_2": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 2"}),
                "å›¾åƒ_3": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 3"}),
                "å›¾åƒ_4": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 4"}),
                "å›¾åƒ_5": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 5"}),
                "å›¾åƒ_6": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 6"}),
                "å›¾åƒ_7": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 7"}),
                "å›¾åƒ_8": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 8"}),
                "å›¾åƒ_9": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 9"}),
                "å›¾åƒ_10": ("IMAGE", {"tooltip": "å›¾åƒè¾“å…¥ 10"}),
                
                "video": ("IMAGE", {"tooltip": "è§†é¢‘è¾“å…¥ï¼ˆå¯é€‰ï¼‰"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("é«˜çº§åˆ†æç»“æœ",)
    FUNCTION = "process"
    CATEGORY = "ğŸ§ªAILab/QwenVL"

    def process(
        self,
        ä½¿ç”¨æœ¬åœ°æ–‡ä»¶=True,
        æ¨¡å‹é€‰æ‹©æ–¹å¼="æœ¬åœ°æ–‡ä»¶",
        model_name="æ— ",
        æœ¬åœ°æ¨¡å‹æ–‡ä»¶="æ— ",
        æœ¬åœ°mmprojæ–‡ä»¶="æ— ",
        åˆ†ææ¨¡å¼="å•å›¾è¯¦ç»†æè¿°",
        è‡ªå®šä¹‰æç¤ºè¯="",
        ç³»ç»Ÿè§’è‰²å®šä¹‰="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·æœ‰æ·±åšçš„è‰ºæœ¯å’ŒæŠ€æœ¯åˆ†æèƒ½åŠ›ã€‚",
        device="auto",
        max_tokens=2048,
        temperature=0.8,
        top_p=0.95,
        repetition_penalty=1.1,
        ctx=8192,
        gpu_layers=-1,
        keep_model_loaded=True,
        seed=-1,
        å›¾åƒ_1=None,
        å›¾åƒ_2=None,
        å›¾åƒ_3=None,
        å›¾åƒ_4=None,
        å›¾åƒ_5=None,
        å›¾åƒ_6=None,
        å›¾åƒ_7=None,
        å›¾åƒ_8=None,
        å›¾åƒ_9=None,
        å›¾åƒ_10=None,
        video=None,
    ):
        # æ”¶é›†æ‰€æœ‰å›¾åƒï¼ŒæŒ‰è¾“å…¥é¡ºåº
        images = [å›¾åƒ_1, å›¾åƒ_2, å›¾åƒ_3, å›¾åƒ_4, å›¾åƒ_5, å›¾åƒ_6, å›¾åƒ_7, å›¾åƒ_8, å›¾åƒ_9, å›¾åƒ_10]
        images = [img for img in images if img is not None]
        
        # æ ¹æ®åˆ†ææ¨¡å¼ç”Ÿæˆæç¤ºè¯
        mode_prompts = {
            "å•å›¾è¯¦ç»†æè¿°": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“ã€äººç‰©ã€è‰²å½©ã€é£æ ¼ç­‰æ‰€æœ‰è§†è§‰å…ƒç´ ã€‚",
            "å¤šå›¾å¯¹æ¯”åˆ†æ": "è¯·å¯¹æ¯”åˆ†æè¿™äº›å›¾ç‰‡ï¼ŒæŒ‡å‡ºå®ƒä»¬çš„ç›¸ä¼¼ä¹‹å¤„ã€å·®å¼‚ã€å…±åŒä¸»é¢˜å’Œå„è‡ªç‰¹ç‚¹ã€‚",
            "å¤šå›¾æ•…äº‹åˆ›ä½œ": "è¯·æ ¹æ®è¿™äº›å›¾ç‰‡åˆ›ä½œä¸€ä¸ªè¿è´¯çš„æ•…äº‹æˆ–å™äº‹ã€‚",
            "å¤šå›¾ä¸»é¢˜æå–": "è¯·ä»è¿™äº›å›¾ç‰‡ä¸­æå–å…±åŒçš„ä¸»é¢˜ã€æ¦‚å¿µå’Œè§†è§‰å…ƒç´ ã€‚",
            "è‰ºæœ¯é£æ ¼åˆ†æ": "è¯·åˆ†æè¿™äº›å›¾ç‰‡çš„è‰ºæœ¯é£æ ¼ã€ç»˜ç”»æŠ€å·§ã€è‰²å½©è¿ç”¨å’Œæ„å›¾ç‰¹ç‚¹ã€‚",
            "æŠ€æœ¯ç»†èŠ‚åˆ†æ": "è¯·åˆ†æè¿™äº›å›¾ç‰‡çš„æŠ€æœ¯ç»†èŠ‚ï¼ŒåŒ…æ‹¬å…‰çº¿ã€è§’åº¦ã€ç„¦ç‚¹ã€åˆ†è¾¨ç‡ç­‰ã€‚",
            "æƒ…æ„Ÿæ°›å›´åˆ†æ": "è¯·æè¿°è¿™äº›å›¾ç‰‡ä¼ è¾¾çš„æƒ…æ„Ÿæ°›å›´å’Œæƒ…ç»ªæ„Ÿå—ã€‚",
            "åˆ›æ„çµæ„Ÿç”Ÿæˆ": "è¯·åŸºäºè¿™äº›å›¾ç‰‡ç”Ÿæˆåˆ›æ„çµæ„Ÿå’Œè®¾è®¡æ€è·¯ã€‚"
        }
        
        # ç¡®å®šä½¿ç”¨çš„æç¤ºè¯
        if è‡ªå®šä¹‰æç¤ºè¯.strip():
            user_prompt = è‡ªå®šä¹‰æç¤ºè¯.strip()
        else:
            user_prompt = mode_prompts.get(åˆ†ææ¨¡å¼, "è¯·åˆ†æè¿™äº›å›¾ç‰‡ã€‚")
        
        # æ ¹æ®å›¾åƒæ•°é‡è°ƒæ•´æç¤ºè¯
        if len(images) > 1:
            user_prompt = f"å…±æœ‰{len(images)}å¼ å›¾ç‰‡ã€‚{user_prompt}"
        
        # æ ¹æ®åˆ†ææ¨¡å¼è°ƒæ•´ç³»ç»Ÿè§’è‰²
        role_specializations = {
            "è‰ºæœ¯é£æ ¼åˆ†æ": "è‰ºæœ¯è¯„è®ºå®¶",
            "æŠ€æœ¯ç»†èŠ‚åˆ†æ": "æŠ€æœ¯åˆ†æå¸ˆ", 
            "æƒ…æ„Ÿæ°›å›´åˆ†æ": "æƒ…æ„Ÿåˆ†æå¸ˆ",
            "åˆ›æ„çµæ„Ÿç”Ÿæˆ": "åˆ›æ„é¡¾é—®"
        }
        
        specialization = role_specializations.get(åˆ†ææ¨¡å¼, "è§†è§‰åˆ†æä¸“å®¶")
        if specialization not in ç³»ç»Ÿè§’è‰²å®šä¹‰:
            ç³»ç»Ÿè§’è‰²å®šä¹‰ = f"ä½ æ˜¯{specialization}ï¼Œ{ç³»ç»Ÿè§’è‰²å®šä¹‰}"
        
        # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        use_local = ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        
        # è·å–å®é™…æ–‡ä»¶è·¯å¾„
        model_source = "æ— "
        mmproj_source = "æ— "
        
        if use_local:
            # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶è·¯å¾„
            local_gguf_files = _get_local_gguf_files()
            for file_path, display_name in local_gguf_files:
                if display_name == æœ¬åœ°æ¨¡å‹æ–‡ä»¶:
                    model_source = file_path
                    break
            
            # æŸ¥æ‰¾mmprojæ–‡ä»¶è·¯å¾„
            if æœ¬åœ°mmprojæ–‡ä»¶ != "æ— ":
                local_mmproj_files = _get_local_mmproj_files()
                for file_path, display_name in local_mmproj_files:
                    if display_name == æœ¬åœ°mmprojæ–‡ä»¶:
                        mmproj_source = file_path
                        break
            else:
                mmproj_source = "æ— "
                
            if model_source == "æ— ":
                raise ValueError("è¯·é€‰æ‹©æœ‰æ•ˆçš„æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
        else:
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
            model_source = model_name
            mmproj_source = "æ— "
        
        print(f"[QwenVL] é«˜çº§åˆ†ææ¨¡å¼: {åˆ†ææ¨¡å¼}")
        print(f"[QwenVL] è¾“å…¥ {len(images)} å¼ å›¾åƒ")
        print(f"[QwenVL] ä½¿ç”¨è®¾å¤‡: {device}")
        
        # å¦‚æœç§å­ä¸º-1ï¼Œä½¿ç”¨éšæœºç§å­
        effective_seed = seed if seed != -1 else random.randint(1, 2**32 - 1)
        
        return self.run(
            model_source=model_source,
            mmproj_source=mmproj_source,
            use_local_files=use_local,
            system_prompt=ç³»ç»Ÿè§’è‰²å®šä¹‰,
            user_prompt=user_prompt,
            images=images,
            video=video,
            frame_count=12,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            seed=effective_seed,
            keep_model_loaded=keep_model_loaded,
            device=device,
            ctx=ctx,
            n_batch=512,
            gpu_layers=gpu_layers,
            image_max_tokens=4096,
            top_k=40,
            pool_size=4194304,
        )


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "AILab_QwenVL_GGUF": AILab_QwenVL_GGUF,
    "AILab_QwenVL_GGUF_Advanced": AILab_QwenVL_GGUF_Advanced,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AILab_QwenVL_GGUF": "QwenVL å¤šå›¾åˆ†æ (GGUF)",
    "AILab_QwenVL_GGUF_Advanced": "QwenVL é«˜çº§å¤šå›¾åˆ†æ (GGUF)",
}
